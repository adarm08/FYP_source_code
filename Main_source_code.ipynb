{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "w3-UG_NHQQK_",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "w3-UG_NHQQK_",
    "outputId": "0c8fe56a-e9c8-4753-c82c-e4e7acc7832a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Requirement already satisfied: yfinance in /usr/local/lib/python3.10/dist-packages (0.2.18)\n",
      "Requirement already satisfied: pandas>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from yfinance) (1.5.3)\n",
      "Requirement already satisfied: numpy>=1.16.5 in /usr/local/lib/python3.10/dist-packages (from yfinance) (1.22.4)\n",
      "Requirement already satisfied: requests>=2.26 in /usr/local/lib/python3.10/dist-packages (from yfinance) (2.27.1)\n",
      "Requirement already satisfied: multitasking>=0.0.7 in /usr/local/lib/python3.10/dist-packages (from yfinance) (0.0.11)\n",
      "Requirement already satisfied: lxml>=4.9.1 in /usr/local/lib/python3.10/dist-packages (from yfinance) (4.9.2)\n",
      "Requirement already satisfied: appdirs>=1.4.4 in /usr/local/lib/python3.10/dist-packages (from yfinance) (1.4.4)\n",
      "Requirement already satisfied: pytz>=2022.5 in /usr/local/lib/python3.10/dist-packages (from yfinance) (2022.7.1)\n",
      "Requirement already satisfied: frozendict>=2.3.4 in /usr/local/lib/python3.10/dist-packages (from yfinance) (2.3.7)\n",
      "Requirement already satisfied: cryptography>=3.3.2 in /usr/local/lib/python3.10/dist-packages (from yfinance) (40.0.2)\n",
      "Requirement already satisfied: beautifulsoup4>=4.11.1 in /usr/local/lib/python3.10/dist-packages (from yfinance) (4.11.2)\n",
      "Requirement already satisfied: html5lib>=1.1 in /usr/local/lib/python3.10/dist-packages (from yfinance) (1.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4>=4.11.1->yfinance) (2.4.1)\n",
      "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography>=3.3.2->yfinance) (1.15.1)\n",
      "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.10/dist-packages (from html5lib>=1.1->yfinance) (1.16.0)\n",
      "Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from html5lib>=1.1->yfinance) (0.5.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.3.0->yfinance) (2.8.2)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26->yfinance) (1.26.15)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26->yfinance) (2022.12.7)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26->yfinance) (2.0.12)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26->yfinance) (3.4)\n",
      "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography>=3.3.2->yfinance) (2.21)\n",
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Collecting ta\n",
      "  Downloading ta-0.10.2.tar.gz (25 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from ta) (1.22.4)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from ta) (1.5.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->ta) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->ta) (2022.7.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->ta) (1.16.0)\n",
      "Building wheels for collected packages: ta\n",
      "  Building wheel for ta (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for ta: filename=ta-0.10.2-py3-none-any.whl size=29088 sha256=ddad015c83df4a7b0a275beb8b327a3e1fc5eeb2c1a9952e1f31437c73b01259\n",
      "  Stored in directory: /root/.cache/pip/wheels/47/51/06/380dc516ea78621870b93ff65527c251afdfdc5fa9d7f4d248\n",
      "Successfully built ta\n",
      "Installing collected packages: ta\n",
      "Successfully installed ta-0.10.2\n",
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Requirement already satisfied: spacy in /usr/local/lib/python3.10/dist-packages (3.5.2)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.4)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.9)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.7)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.8)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy) (8.1.9)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.1.1)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.4.6)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.8)\n",
      "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.7.0)\n",
      "Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.10.1)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (6.3.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (4.65.0)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.22.4)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.27.1)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.10.7)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.1.2)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy) (67.7.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (23.1)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.3.0)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy) (4.5.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.15)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2022.12.7)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.12)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy) (0.7.9)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy) (0.0.4)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.8.0,>=0.3.0->spacy) (8.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy) (2.1.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install yfinance\n",
    "!pip install ta\n",
    "!pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f1ebdb33",
   "metadata": {
    "id": "f1ebdb33"
   },
   "outputs": [],
   "source": [
    "# required libraries\n",
    "import json\n",
    "import requests\n",
    "import pandas as pd\n",
    "from datetime import date, timedelta\n",
    "import collections\n",
    "from nltk import tokenize\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk import sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import numpy as np\n",
    "import yfinance as yf\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import time\n",
    "import pytz\n",
    "import datetime\n",
    "import torch\n",
    "import sklearn.feature_extraction\n",
    "import sklearn.model_selection\n",
    "import random\n",
    "from sklearn.metrics import accuracy_score \n",
    "from sklearn import svm\n",
    "from sklearn import metrics\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import ta\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "import spacy\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)\n",
    "from sklearn.preprocessing import normalize\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ca298bc4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ca298bc4",
    "outputId": "4bd0526f-53d4-4435-8ade-d47f1b652084"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\andre\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\andre\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('vader_lexicon')\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2m06EiuNQyzr",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2m06EiuNQyzr",
    "outputId": "0082db8e-c46e-464e-ba71-b33f7884d877"
   },
   "outputs": [],
   "source": [
    "#from google.colab import drive\n",
    "#drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b95e2cae",
   "metadata": {
    "id": "b95e2cae"
   },
   "outputs": [],
   "source": [
    "# Set the time period \n",
    "# Time period of 10 years (2013-2023)\n",
    "specific_date = pd.to_datetime('2013-01-01')\n",
    "specific_date_time = date(2013, 1, 1)\n",
    "specific_end_date = pd.to_datetime('2023-01-01')\n",
    "start_date = date(2012, 11, 1)\n",
    "end_date = date(2023, 1, 10) \n",
    "delta = date(2023, 1, 1) - specific_date_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b59bb114",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 618
    },
    "id": "b59bb114",
    "outputId": "db23f633-b7cd-498b-dd12-cbf87681505a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1000x700 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get the data for the Stock/Index (this case for the S&P 500 ticker '^GSPC')\n",
    "ticker = '^GSPC'  \n",
    "stock_data = yf.download(ticker, start_date, end_date)\n",
    "stock_data['daily_returns'] =  ((stock_data['Close'] - stock_data['Open']) / stock_data['Open']) * 100\n",
    "\n",
    "# Plot the Close price\n",
    "stock_data['Close'].plot(figsize=(10, 7))\n",
    "\n",
    "plt.title(\"Close Price of {}\".format(ticker), fontsize=16)\n",
    "plt.ylabel('Price', fontsize=14)\n",
    "plt.xlabel('Date', fontsize=14)\n",
    "plt.grid(which=\"major\", color='k', linestyle='-.', linewidth=0.5)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3891dd3e",
   "metadata": {
    "id": "3891dd3e"
   },
   "outputs": [],
   "source": [
    "# Pre-processing the data\n",
    "# Arrange the dataset to get the Daily Returns\n",
    "stock_daily_returns = ((stock_data['Close'] - stock_data['Open']) / stock_data['Open']) * 100\n",
    "stock_returns = pd.DataFrame(stock_daily_returns)\n",
    "stock_returns.columns = ['Return']\n",
    "stock_returns = stock_returns.reset_index()\n",
    "\n",
    "# Days where the stock market is open\n",
    "tradingDays = pd.DataFrame(stock_returns['Date']).astype(str)\n",
    "# Arrange index\n",
    "stock_data[\"Date\"] = stock_data.index\n",
    "stock_data.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# get list of trading days so we can use .index()\n",
    "listOfTradingDays = tradingDays.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "786f43b7",
   "metadata": {
    "id": "786f43b7"
   },
   "outputs": [],
   "source": [
    "# Helper functions to build the dataset\n",
    "\n",
    "# Function to get 1-day's stock performance\n",
    "def get1DayReturn(date):\n",
    "    dayReturn = stock_returns.where(stock_returns['Date'] == date)\n",
    "    dayReturn = dayReturn.dropna()\n",
    "    return dayReturn['Return'].iloc[0]\n",
    "\n",
    "# Function to get tommo's open\n",
    "def getTommoOpen(date):\n",
    "    indexOfTommoDay = listOfTradingDays.index([date]) + 1\n",
    "    if indexOfTommoDay >= len(listOfTradingDays):\n",
    "        return -1\n",
    "    date2 = listOfTradingDays[indexOfTommoDay][0]\n",
    "    return stock_data['Open'].where(stock_data['Date'] == date2).dropna().iloc[0]\n",
    "\n",
    "# Function to get stock direction from Open - > Close of day n\n",
    "def getStockDirection(date, n_day):\n",
    "    dayOpen = getTommoOpen(date)\n",
    "    indexOfDay = listOfTradingDays.index([date]) + n_day\n",
    "    \n",
    "    if indexOfDay >= len(listOfTradingDays):\n",
    "        return -1\n",
    "    \n",
    "    dateClose = listOfTradingDays[indexOfDay][0]\n",
    "    dayClose = stock_data['Adj Close'].where(stock_data['Date'] == dateClose).dropna().iloc[0]\n",
    "    if dayClose >= dayOpen: # if close > open then return 1 'Up' \n",
    "        return 1\n",
    "    else: \n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0eca0590",
   "metadata": {
    "id": "0eca0590"
   },
   "outputs": [],
   "source": [
    "# Building the Stock prices Dataset\n",
    "Stock_info = pd.DataFrame({'Today return' : [], 'Tommo_Open' : [], 'Tommo_Direction' : [], '2Day_Direction' : [], '3Day_Direction' : [], '4Day_Direction' : [], '5Day_Direction' : []})\n",
    "for d in stock_data['Date']:\n",
    "    Date = d\n",
    "    date_s = str(d)[0:10]\n",
    "    new_row = {'Today return': get1DayReturn(d), 'Tommo_Open' : getTommoOpen(date_s) , 'Tommo_Direction': getStockDirection(date_s, 1),'2Day_Direction' : getStockDirection(date_s, 2), '3Day_Direction': getStockDirection(date_s, 3),'4Day_Direction' : getStockDirection(date_s, 4), '5Day_Direction':getStockDirection(date_s, 5)}\n",
    "    Stock_info = Stock_info.append(new_row, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2b743f80",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 678
    },
    "id": "2b743f80",
    "outputId": "52395f93-5cab-477b-eca8-cd618d1e821e"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Date</th>\n",
       "      <th>Today return</th>\n",
       "      <th>Tommo_Open</th>\n",
       "      <th>Tommo_Direction</th>\n",
       "      <th>2Day_Direction</th>\n",
       "      <th>3Day_Direction</th>\n",
       "      <th>4Day_Direction</th>\n",
       "      <th>5Day_Direction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1412.199951</td>\n",
       "      <td>1428.349976</td>\n",
       "      <td>1412.199951</td>\n",
       "      <td>1427.589966</td>\n",
       "      <td>1427.589966</td>\n",
       "      <td>3929890000</td>\n",
       "      <td>2012-11-01</td>\n",
       "      <td>1.089790</td>\n",
       "      <td>1427.589966</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1427.589966</td>\n",
       "      <td>1434.270020</td>\n",
       "      <td>1412.910034</td>\n",
       "      <td>1414.199951</td>\n",
       "      <td>1414.199951</td>\n",
       "      <td>3732480000</td>\n",
       "      <td>2012-11-02</td>\n",
       "      <td>-0.937945</td>\n",
       "      <td>1414.020020</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1414.020020</td>\n",
       "      <td>1419.900024</td>\n",
       "      <td>1408.130005</td>\n",
       "      <td>1417.260010</td>\n",
       "      <td>1417.260010</td>\n",
       "      <td>2921040000</td>\n",
       "      <td>2012-11-05</td>\n",
       "      <td>0.229133</td>\n",
       "      <td>1417.260010</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1417.260010</td>\n",
       "      <td>1433.380005</td>\n",
       "      <td>1417.260010</td>\n",
       "      <td>1428.390015</td>\n",
       "      <td>1428.390015</td>\n",
       "      <td>3306970000</td>\n",
       "      <td>2012-11-06</td>\n",
       "      <td>0.785318</td>\n",
       "      <td>1428.270020</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1428.270020</td>\n",
       "      <td>1428.270020</td>\n",
       "      <td>1388.140015</td>\n",
       "      <td>1394.530029</td>\n",
       "      <td>1394.530029</td>\n",
       "      <td>4356490000</td>\n",
       "      <td>2012-11-07</td>\n",
       "      <td>-2.362298</td>\n",
       "      <td>1394.530029</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2559</th>\n",
       "      <td>3853.290039</td>\n",
       "      <td>3878.459961</td>\n",
       "      <td>3794.330078</td>\n",
       "      <td>3824.139893</td>\n",
       "      <td>3824.139893</td>\n",
       "      <td>3959140000</td>\n",
       "      <td>2023-01-03</td>\n",
       "      <td>-0.756500</td>\n",
       "      <td>3840.360107</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2560</th>\n",
       "      <td>3840.360107</td>\n",
       "      <td>3873.159912</td>\n",
       "      <td>3815.770020</td>\n",
       "      <td>3852.969971</td>\n",
       "      <td>3852.969971</td>\n",
       "      <td>4414080000</td>\n",
       "      <td>2023-01-04</td>\n",
       "      <td>0.328351</td>\n",
       "      <td>3839.739990</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2561</th>\n",
       "      <td>3839.739990</td>\n",
       "      <td>3839.739990</td>\n",
       "      <td>3802.419922</td>\n",
       "      <td>3808.100098</td>\n",
       "      <td>3808.100098</td>\n",
       "      <td>3893450000</td>\n",
       "      <td>2023-01-05</td>\n",
       "      <td>-0.824011</td>\n",
       "      <td>3823.370117</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2562</th>\n",
       "      <td>3823.370117</td>\n",
       "      <td>3906.189941</td>\n",
       "      <td>3809.560059</td>\n",
       "      <td>3895.080078</td>\n",
       "      <td>3895.080078</td>\n",
       "      <td>3923560000</td>\n",
       "      <td>2023-01-06</td>\n",
       "      <td>1.875569</td>\n",
       "      <td>3910.820068</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2563</th>\n",
       "      <td>3910.820068</td>\n",
       "      <td>3950.570068</td>\n",
       "      <td>3890.419922</td>\n",
       "      <td>3892.090088</td>\n",
       "      <td>3892.090088</td>\n",
       "      <td>4311770000</td>\n",
       "      <td>2023-01-09</td>\n",
       "      <td>-0.478927</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2564 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             Open         High          Low        Close    Adj Close  \\\n",
       "0     1412.199951  1428.349976  1412.199951  1427.589966  1427.589966   \n",
       "1     1427.589966  1434.270020  1412.910034  1414.199951  1414.199951   \n",
       "2     1414.020020  1419.900024  1408.130005  1417.260010  1417.260010   \n",
       "3     1417.260010  1433.380005  1417.260010  1428.390015  1428.390015   \n",
       "4     1428.270020  1428.270020  1388.140015  1394.530029  1394.530029   \n",
       "...           ...          ...          ...          ...          ...   \n",
       "2559  3853.290039  3878.459961  3794.330078  3824.139893  3824.139893   \n",
       "2560  3840.360107  3873.159912  3815.770020  3852.969971  3852.969971   \n",
       "2561  3839.739990  3839.739990  3802.419922  3808.100098  3808.100098   \n",
       "2562  3823.370117  3906.189941  3809.560059  3895.080078  3895.080078   \n",
       "2563  3910.820068  3950.570068  3890.419922  3892.090088  3892.090088   \n",
       "\n",
       "          Volume       Date  Today return   Tommo_Open  Tommo_Direction  \\\n",
       "0     3929890000 2012-11-01      1.089790  1427.589966              0.0   \n",
       "1     3732480000 2012-11-02     -0.937945  1414.020020              1.0   \n",
       "2     2921040000 2012-11-05      0.229133  1417.260010              1.0   \n",
       "3     3306970000 2012-11-06      0.785318  1428.270020              0.0   \n",
       "4     4356490000 2012-11-07     -2.362298  1394.530029              0.0   \n",
       "...          ...        ...           ...          ...              ...   \n",
       "2559  3959140000 2023-01-03     -0.756500  3840.360107              1.0   \n",
       "2560  4414080000 2023-01-04      0.328351  3839.739990              0.0   \n",
       "2561  3893450000 2023-01-05     -0.824011  3823.370117              1.0   \n",
       "2562  3923560000 2023-01-06      1.875569  3910.820068              0.0   \n",
       "2563  4311770000 2023-01-09     -0.478927    -1.000000             -1.0   \n",
       "\n",
       "      2Day_Direction  3Day_Direction  4Day_Direction  5Day_Direction  \n",
       "0                0.0             1.0             0.0             0.0  \n",
       "1                1.0             0.0             0.0             0.0  \n",
       "2                0.0             0.0             0.0             0.0  \n",
       "3                0.0             0.0             0.0             0.0  \n",
       "4                0.0             0.0             0.0             0.0  \n",
       "...              ...             ...             ...             ...  \n",
       "2559             0.0             1.0             1.0            -1.0  \n",
       "2560             1.0             1.0            -1.0            -1.0  \n",
       "2561             1.0            -1.0            -1.0            -1.0  \n",
       "2562            -1.0            -1.0            -1.0            -1.0  \n",
       "2563            -1.0            -1.0            -1.0            -1.0  \n",
       "\n",
       "[2564 rows x 14 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# add this data to the stock_data dataframe\n",
    "stock_data.drop(columns='daily_returns', inplace=True)\n",
    "stock_data = pd.merge(stock_data, Stock_info, left_index=True, right_index=True)\n",
    "stock_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "gAJn2Qj6L0_w",
   "metadata": {
    "id": "gAJn2Qj6L0_w"
   },
   "outputs": [],
   "source": [
    "# Technical indicator part: Compute and add chosen technical indicators uisng ta library\n",
    "# Technical indicators similar to Mehar Vijh et al. Paper\n",
    "\n",
    "# High - Low\n",
    "stock_data['high-low'] = stock_data['High'] - stock_data['Low']\n",
    "\n",
    "# Open - Close\n",
    "stock_data['open-close'] = stock_data['Open'] - stock_data['Close']\n",
    "\n",
    "# Simple moving average\n",
    "stock_data['sma_7'] = ta.trend.sma_indicator(close=stock_data['Close'], window=7)\n",
    "stock_data['sma_14'] = ta.trend.sma_indicator(close=stock_data['Close'], window=14)\n",
    "stock_data['sma_21'] = ta.trend.sma_indicator(close=stock_data['Close'], window=21)\n",
    "\n",
    "# Standard Deviation\n",
    "stock_data['std'] = ta.volatility.bollinger_mavg(stock_data['Close'], window=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "kXsiiat7L93o",
   "metadata": {
    "id": "kXsiiat7L93o"
   },
   "outputs": [],
   "source": [
    "# Technical indicators similar to Osman Hegazy et al.\n",
    "\n",
    "# RSI (Relative Strength Index)\n",
    "stock_data['rsi_7'] = ta.momentum.RSIIndicator(stock_data['Close'], 7).rsi()\n",
    "\n",
    "# Money Flow Index (MFI)\n",
    "stock_data['mfi_7'] = ta.volume.money_flow_index(high=stock_data['High'], low=stock_data['Low'], close=stock_data['Close'], volume=stock_data['Volume'], window=7)\n",
    "\n",
    "# Exponential Moving Average (EMA)\n",
    "stock_data['ema_7'] = ta.trend.ema_indicator(close=stock_data['Close'], window=7)\n",
    "\n",
    "# Stochastic Oscillator (SO)\n",
    "stoch = ta.momentum.StochasticOscillator(high=stock_data['High'], low=stock_data['Low'], close=stock_data['Close'])\n",
    "stock_data['so_%K'] = stoch.stoch()\n",
    "stock_data['so_%D'] = stoch.stoch_signal()\n",
    "\n",
    "# Moving Average Convergence Divergence (MACD)\n",
    "macd = ta.trend.MACD(close=stock_data['Close'])\n",
    "stock_data['macd'] = macd.macd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "16a5f5a9",
   "metadata": {
    "id": "16a5f5a9"
   },
   "outputs": [],
   "source": [
    "# Technical indicators similar to Sanbo Wang (best found technical model)\n",
    "stock_data['sma'] = ta.trend.sma_indicator(close=stock_data['Close'], window=10)\n",
    "\n",
    "\n",
    "# Bollinger Bands (BBANDS), use the average.\n",
    "bb = ta.volatility.BollingerBands(close=stock_data['Close'], window=20, window_dev=2)\n",
    "stock_data['bb_middleband'] = bb.bollinger_mavg()\n",
    "\n",
    "# AROON\n",
    "ind = ta.trend.AroonIndicator(close =stock_data['Close'], window=25)\n",
    "stock_data['ar_up'] =  ind.aroon_up() \n",
    "stock_data['ar_down'] = ind.aroon_down()\n",
    "\n",
    "# Commodity Channel Index (CCI)\n",
    "stock_data['cci'] = ta.trend.CCIIndicator(high=stock_data['High'], low=stock_data['Low'], close=stock_data['Close'], window=10).cci()\n",
    "\n",
    "# Chande Momentum Oscillator (CMO) (calculating it manually since, library ta does not have it)\n",
    "# Set the period and calculate the difference between the current price and the price 10 periods ago\n",
    "price_diff = stock_data['Close'] - stock_data['Close'].shift(10)\n",
    "gain = np.where(price_diff > 0, price_diff, 0)\n",
    "gain_sum = pd.Series(gain).rolling(window=10).sum()\n",
    "loss = np.where(price_diff < 0, abs(price_diff), 0)\n",
    "loss_sum = pd.Series(loss).rolling(window=10).sum()\n",
    "stock_data['cmo'] = ((gain_sum - loss_sum) / (gain_sum + loss_sum)).fillna(0)\n",
    "\n",
    "# Moving Average Convergence Divergence (MACD)\n",
    "macd = ta.trend.MACD(close=stock_data['Close'])\n",
    "stock_data['macd'] = macd.macd()\n",
    "\n",
    "# RSI (Relative Strength Index)\n",
    "stock_data['rsi'] = ta.momentum.RSIIndicator(stock_data['Close'], 10).rsi()\n",
    "\n",
    "# Stochastic Oscillator (SO)\n",
    "stoch = ta.momentum.StochasticOscillator(high=stock_data['High'], low=stock_data['Low'], close=stock_data['Close'])\n",
    "stock_data['so_%K'] = stoch.stoch()\n",
    "stock_data['so_%D'] = stoch.stoch_signal()\n",
    "\n",
    "# Williams %R (WILLR)\n",
    "stock_data['willr'] = ta.momentum.WilliamsRIndicator(high=stock_data['High'], low=stock_data['Low'], close=stock_data['Close'], lbp=10).williams_r()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "has8qXpdMoov",
   "metadata": {
    "id": "has8qXpdMoov"
   },
   "outputs": [],
   "source": [
    "# Technical indicators similar to Taylan Kabban (For Hybrid model)\n",
    "\n",
    "# RSI (Relative Strength Index)\n",
    "stock_data['rsi'] = ta.momentum.RSIIndicator(stock_data['Close'], 14).rsi()\n",
    "\n",
    "# Stochastic Oscillator (SO)\n",
    "stoch = ta.momentum.StochasticOscillator(high=stock_data['High'], low=stock_data['Low'], close=stock_data['Close'], window=14)\n",
    "stock_data['%K'] = stoch.stoch()\n",
    "\n",
    "\n",
    "# Simple moving average\n",
    "stock_data['sma'] = ta.trend.sma_indicator(close=stock_data['Close'], window=14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b087a298",
   "metadata": {
    "id": "b087a298"
   },
   "outputs": [],
   "source": [
    "# Pre-processing and cleaning\n",
    "# Keep the need trading days\n",
    "stock_data = stock_data[stock_data['Date'] > specific_date]\n",
    "stock_data = stock_data[stock_data['Date'] < specific_end_date]\n",
    "\n",
    "# reset index\n",
    "stock_data = stock_data.reset_index()\n",
    "del stock_data['index']\n",
    "\n",
    "# delete last row (since we always predict next day)\n",
    "stock_data = stock_data.iloc[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c41nQCP_WcBO",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 678
    },
    "id": "c41nQCP_WcBO",
    "outputId": "b493b54c-801f-45e2-890e-d3c787b2cbf7"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Date</th>\n",
       "      <th>Today return</th>\n",
       "      <th>Tommo_Open</th>\n",
       "      <th>Tommo_Direction</th>\n",
       "      <th>...</th>\n",
       "      <th>macd</th>\n",
       "      <th>sma</th>\n",
       "      <th>bb_middleband</th>\n",
       "      <th>ar_up</th>\n",
       "      <th>ar_down</th>\n",
       "      <th>cci</th>\n",
       "      <th>cmo</th>\n",
       "      <th>rsi</th>\n",
       "      <th>willr</th>\n",
       "      <th>%K</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1426.189941</td>\n",
       "      <td>1462.430054</td>\n",
       "      <td>1426.189941</td>\n",
       "      <td>1462.420044</td>\n",
       "      <td>1462.420044</td>\n",
       "      <td>4202600000</td>\n",
       "      <td>2013-01-02</td>\n",
       "      <td>2.540342</td>\n",
       "      <td>1462.420044</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>6.376922</td>\n",
       "      <td>1428.852853</td>\n",
       "      <td>1424.933496</td>\n",
       "      <td>100.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>121.230077</td>\n",
       "      <td>0.639208</td>\n",
       "      <td>64.471728</td>\n",
       "      <td>-0.015562</td>\n",
       "      <td>99.984438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1462.420044</td>\n",
       "      <td>1465.469971</td>\n",
       "      <td>1455.530029</td>\n",
       "      <td>1459.369995</td>\n",
       "      <td>1459.369995</td>\n",
       "      <td>3829730000</td>\n",
       "      <td>2013-01-03</td>\n",
       "      <td>-0.208562</td>\n",
       "      <td>1459.369995</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>8.372865</td>\n",
       "      <td>1431.059283</td>\n",
       "      <td>1427.549493</td>\n",
       "      <td>96.0</td>\n",
       "      <td>88.0</td>\n",
       "      <td>143.976800</td>\n",
       "      <td>0.581270</td>\n",
       "      <td>63.102256</td>\n",
       "      <td>-9.055785</td>\n",
       "      <td>90.944215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1459.369995</td>\n",
       "      <td>1467.939941</td>\n",
       "      <td>1458.989990</td>\n",
       "      <td>1466.469971</td>\n",
       "      <td>1466.469971</td>\n",
       "      <td>3424290000</td>\n",
       "      <td>2013-01-04</td>\n",
       "      <td>0.486510</td>\n",
       "      <td>1466.469971</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>10.407601</td>\n",
       "      <td>1434.417855</td>\n",
       "      <td>1430.408990</td>\n",
       "      <td>100.0</td>\n",
       "      <td>84.0</td>\n",
       "      <td>127.382281</td>\n",
       "      <td>0.591251</td>\n",
       "      <td>64.967721</td>\n",
       "      <td>-2.105072</td>\n",
       "      <td>97.894928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1466.469971</td>\n",
       "      <td>1466.469971</td>\n",
       "      <td>1456.619995</td>\n",
       "      <td>1461.890015</td>\n",
       "      <td>1461.890015</td>\n",
       "      <td>3304970000</td>\n",
       "      <td>2013-01-07</td>\n",
       "      <td>-0.312312</td>\n",
       "      <td>1461.890015</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>11.517809</td>\n",
       "      <td>1437.868574</td>\n",
       "      <td>1432.806494</td>\n",
       "      <td>96.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>92.233405</td>\n",
       "      <td>0.562057</td>\n",
       "      <td>62.763360</td>\n",
       "      <td>-8.663799</td>\n",
       "      <td>91.336201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1461.890015</td>\n",
       "      <td>1461.890015</td>\n",
       "      <td>1451.640015</td>\n",
       "      <td>1457.150024</td>\n",
       "      <td>1457.150024</td>\n",
       "      <td>3601600000</td>\n",
       "      <td>2013-01-08</td>\n",
       "      <td>-0.324237</td>\n",
       "      <td>1457.150024</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>11.878254</td>\n",
       "      <td>1439.782148</td>\n",
       "      <td>1434.760498</td>\n",
       "      <td>92.0</td>\n",
       "      <td>76.0</td>\n",
       "      <td>60.821371</td>\n",
       "      <td>0.599050</td>\n",
       "      <td>60.476332</td>\n",
       "      <td>-15.451702</td>\n",
       "      <td>84.548298</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 35 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          Open         High          Low        Close    Adj Close  \\\n",
       "0  1426.189941  1462.430054  1426.189941  1462.420044  1462.420044   \n",
       "1  1462.420044  1465.469971  1455.530029  1459.369995  1459.369995   \n",
       "2  1459.369995  1467.939941  1458.989990  1466.469971  1466.469971   \n",
       "3  1466.469971  1466.469971  1456.619995  1461.890015  1461.890015   \n",
       "4  1461.890015  1461.890015  1451.640015  1457.150024  1457.150024   \n",
       "\n",
       "       Volume       Date  Today return   Tommo_Open  Tommo_Direction  ...  \\\n",
       "0  4202600000 2013-01-02      2.540342  1462.420044              0.0  ...   \n",
       "1  3829730000 2013-01-03     -0.208562  1459.369995              1.0  ...   \n",
       "2  3424290000 2013-01-04      0.486510  1466.469971              0.0  ...   \n",
       "3  3304970000 2013-01-07     -0.312312  1461.890015              0.0  ...   \n",
       "4  3601600000 2013-01-08     -0.324237  1457.150024              1.0  ...   \n",
       "\n",
       "        macd          sma  bb_middleband  ar_up  ar_down         cci  \\\n",
       "0   6.376922  1428.852853    1424.933496  100.0      4.0  121.230077   \n",
       "1   8.372865  1431.059283    1427.549493   96.0     88.0  143.976800   \n",
       "2  10.407601  1434.417855    1430.408990  100.0     84.0  127.382281   \n",
       "3  11.517809  1437.868574    1432.806494   96.0     80.0   92.233405   \n",
       "4  11.878254  1439.782148    1434.760498   92.0     76.0   60.821371   \n",
       "\n",
       "        cmo        rsi      willr         %K  \n",
       "0  0.639208  64.471728  -0.015562  99.984438  \n",
       "1  0.581270  63.102256  -9.055785  90.944215  \n",
       "2  0.591251  64.967721  -2.105072  97.894928  \n",
       "3  0.562057  62.763360  -8.663799  91.336201  \n",
       "4  0.599050  60.476332 -15.451702  84.548298  \n",
       "\n",
       "[5 rows x 35 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stock_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d7d0e176",
   "metadata": {
    "id": "d7d0e176"
   },
   "outputs": [],
   "source": [
    "# remove certain features\n",
    "final_dataset = stock_data.drop(columns=['Date','Tommo_Direction','2Day_Direction', '3Day_Direction', '4Day_Direction', '5Day_Direction'])\n",
    "\n",
    "# Using StandardScaler to normalise the values before passing them to the Neural network\n",
    "scaler = StandardScaler()\n",
    "normalisedvalues = scaler.fit_transform(final_dataset.values)\n",
    "normalised_dataset = pd.DataFrame(columns = final_dataset.columns, data = normalisedvalues, index = final_dataset.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "eb498ae5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eb498ae5",
    "outputId": "212224e1-7c18-4a06-f62c-2a6bfab899bc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of instances:  2517\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of instances: \", len(normalised_dataset)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "143cc589",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 363
    },
    "id": "143cc589",
    "outputId": "3760f481-d119-4873-a9e4-ce978f30e9d6"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Headline</th>\n",
       "      <th>Abstract</th>\n",
       "      <th>Source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2013-01-01 23:46:43+00:00</td>\n",
       "      <td>New York’s Office Builders Raise Their Online ...</td>\n",
       "      <td>The marketing teams behind New York office bui...</td>\n",
       "      <td>The New York Times</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2013-01-01 22:35:07+00:00</td>\n",
       "      <td>Bigger Tax Bite for Most Under Fiscal Pact</td>\n",
       "      <td>Although a higher income tax rate will apply o...</td>\n",
       "      <td>The New York Times</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2013-01-01 22:21:06+00:00</td>\n",
       "      <td>Biotech Players Lead a Boom in Cambridge</td>\n",
       "      <td>The presence of M.I.T. and Harvard, along with...</td>\n",
       "      <td>The New York Times</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2013-01-01 19:42:25+00:00</td>\n",
       "      <td>Malls Blossom in Russia, With a Middle Class</td>\n",
       "      <td>While malls appear to be past their peak in th...</td>\n",
       "      <td>The New York Times</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2013-01-01 08:29:10+00:00</td>\n",
       "      <td>Duke Seeks Final Approval for Its Campus in China</td>\n",
       "      <td>Duke University aims to submit its application...</td>\n",
       "      <td>International Herald Tribune</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58580</th>\n",
       "      <td>2023-04-28 10:00:31+00:00</td>\n",
       "      <td>The Eurozone Economy Shows Signs of Modest Growth</td>\n",
       "      <td>The countries that use the euro recorded econo...</td>\n",
       "      <td>The New York Times</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58581</th>\n",
       "      <td>2023-04-28 09:00:36+00:00</td>\n",
       "      <td>Hollywood, Both Frantic and Calm, Braces for W...</td>\n",
       "      <td>Studios have moved up deadlines for TV writers...</td>\n",
       "      <td>The New York Times</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58582</th>\n",
       "      <td>2023-04-28 09:00:34+00:00</td>\n",
       "      <td>Higher Food Prices Bring Bigger Profits, but C...</td>\n",
       "      <td>Some of the biggest packaged food companies ra...</td>\n",
       "      <td>The New York Times</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58583</th>\n",
       "      <td>2023-04-28 09:00:23+00:00</td>\n",
       "      <td>A ‘Rocky and Bumpy’ Economy Where Wages Are Up...</td>\n",
       "      <td>Key pay and inflation gauges have stayed stubb...</td>\n",
       "      <td>The New York Times</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58584</th>\n",
       "      <td>2023-04-28 09:00:15+00:00</td>\n",
       "      <td>Fed Slams Its Own Oversight of Silicon Valley ...</td>\n",
       "      <td>The Federal Reserve released hundreds of pages...</td>\n",
       "      <td>The New York Times</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>58585 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                           Date  \\\n",
       "0     2013-01-01 23:46:43+00:00   \n",
       "1     2013-01-01 22:35:07+00:00   \n",
       "2     2013-01-01 22:21:06+00:00   \n",
       "3     2013-01-01 19:42:25+00:00   \n",
       "4     2013-01-01 08:29:10+00:00   \n",
       "...                         ...   \n",
       "58580 2023-04-28 10:00:31+00:00   \n",
       "58581 2023-04-28 09:00:36+00:00   \n",
       "58582 2023-04-28 09:00:34+00:00   \n",
       "58583 2023-04-28 09:00:23+00:00   \n",
       "58584 2023-04-28 09:00:15+00:00   \n",
       "\n",
       "                                                Headline  \\\n",
       "0      New York’s Office Builders Raise Their Online ...   \n",
       "1             Bigger Tax Bite for Most Under Fiscal Pact   \n",
       "2               Biotech Players Lead a Boom in Cambridge   \n",
       "3           Malls Blossom in Russia, With a Middle Class   \n",
       "4      Duke Seeks Final Approval for Its Campus in China   \n",
       "...                                                  ...   \n",
       "58580  The Eurozone Economy Shows Signs of Modest Growth   \n",
       "58581  Hollywood, Both Frantic and Calm, Braces for W...   \n",
       "58582  Higher Food Prices Bring Bigger Profits, but C...   \n",
       "58583  A ‘Rocky and Bumpy’ Economy Where Wages Are Up...   \n",
       "58584  Fed Slams Its Own Oversight of Silicon Valley ...   \n",
       "\n",
       "                                                Abstract  \\\n",
       "0      The marketing teams behind New York office bui...   \n",
       "1      Although a higher income tax rate will apply o...   \n",
       "2      The presence of M.I.T. and Harvard, along with...   \n",
       "3      While malls appear to be past their peak in th...   \n",
       "4      Duke University aims to submit its application...   \n",
       "...                                                  ...   \n",
       "58580  The countries that use the euro recorded econo...   \n",
       "58581  Studios have moved up deadlines for TV writers...   \n",
       "58582  Some of the biggest packaged food companies ra...   \n",
       "58583  Key pay and inflation gauges have stayed stubb...   \n",
       "58584  The Federal Reserve released hundreds of pages...   \n",
       "\n",
       "                             Source  \n",
       "0                The New York Times  \n",
       "1                The New York Times  \n",
       "2                The New York Times  \n",
       "3                The New York Times  \n",
       "4      International Herald Tribune  \n",
       "...                             ...  \n",
       "58580            The New York Times  \n",
       "58581            The New York Times  \n",
       "58582            The New York Times  \n",
       "58583            The New York Times  \n",
       "58584            The New York Times  \n",
       "\n",
       "[58585 rows x 4 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# News Data from NYT (to be processed using NLP methods) (this part can be skiped)*\n",
    "# Load news data for period 2013-2023\n",
    "news_data = pd.read_feather(\"dataset\\\\news_data_2013-2023.feather\")\n",
    "news_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "92d727bf",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 235
    },
    "id": "92d727bf",
    "outputId": "74860a4e-4af8-4474-8ecc-98b504ebc64e"
   },
   "outputs": [],
   "source": [
    "# keep only date and article columns\n",
    "news_data = news_data.drop(columns=['Source']) # remove Source\n",
    "# convert date column to pd.datetime\n",
    "news_data['Date'] = pd.to_datetime(news_data['Date'])\n",
    "\n",
    "article_dataset = pd.DataFrame({\n",
    "    'Date': news_data['Date'].dt.date.astype(str),\n",
    "    'Time': news_data['Date'].dt.time,\n",
    "    'Headline': news_data['Headline'],\n",
    "    'Abstract': news_data['Abstract']\n",
    "})\n",
    "article_dataset['Text'] = article_dataset['Headline'] + ' ' + article_dataset['Abstract'] # column that we will use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f186960c",
   "metadata": {
    "id": "f186960c"
   },
   "outputs": [],
   "source": [
    "# function to get open and close time of the US stock market according to EDT OR EST\n",
    "def get_US_market_open_close_time(date):\n",
    "    eastern_tz = pytz.timezone('US/Eastern')\n",
    "    naive_date = datetime.datetime.strptime(date, '%Y-%m-%d')\n",
    "    aware_date = eastern_tz.localize(naive_date, is_dst=None)\n",
    "    if aware_date.dst() == datetime.timedelta(0):\n",
    "        # case EST\n",
    "        open_time = '14:30:00'\n",
    "        close_time = '21:00:00'\n",
    "    else:\n",
    "        # case EDT\n",
    "        open_time = '13:30:00'\n",
    "        close_time = '20:00:00'\n",
    "        \n",
    "    return open_time, close_time\n",
    "\n",
    "# get Articles takes two parameters date and mode('b' before market open and 'a' after market open)\n",
    "# time of articles is in UTC\n",
    "def getArticles(date, mode):\n",
    "    open_time, close_time = get_US_market_open_close_time(date)\n",
    "    news = article_dataset.where(article_dataset['Date'] == date).dropna()\n",
    "    # convert format (from string to time)\n",
    "    time2 = datetime.datetime.strptime(open_time, '%H:%M:%S').time()\n",
    "    if mode == 'b':\n",
    "        # Get news before Open\n",
    "        news = news.where(news['Time'] < time2).dropna()\n",
    "    elif mode == 'a':\n",
    "        # Get news after close\n",
    "        news = news.where(news['Time'] > time2).dropna()\n",
    "        \n",
    "    if len(news) == 0: # Just in Case\n",
    "        return ''\n",
    "\n",
    "    return ' '.join(news[\"Text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0159143d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 235
    },
    "id": "0159143d",
    "outputId": "0eb114ce-d467-4da9-b7be-871c32812a04"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>NewsHeadlines</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2013-01-02</td>\n",
       "      <td>New York’s Office Builders Raise Their Online ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2013-01-03</td>\n",
       "      <td>A Gigantic Sigh of Relief as Tax Uncertainty E...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2013-01-04</td>\n",
       "      <td>Rig Owner Will Settle With U.S. in Gulf Spill ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2013-01-07</td>\n",
       "      <td>Europe Likely to Be Harder on Google Over Sear...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2013-01-08</td>\n",
       "      <td>Avis and Hertz Acquisitions Raise Questions Ov...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2513</th>\n",
       "      <td>2022-12-23</td>\n",
       "      <td>Twitter Is Said to Have Struggled Over Reveali...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2514</th>\n",
       "      <td>2022-12-27</td>\n",
       "      <td>Alameda Executive Says She Is ‘Truly Sorry’ fo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2515</th>\n",
       "      <td>2022-12-28</td>\n",
       "      <td>Thousands Will Live Here One Day (as Long as T...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2516</th>\n",
       "      <td>2022-12-29</td>\n",
       "      <td>10 Months Into Job, Southwest’s C.E.O. Faces a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2517</th>\n",
       "      <td>2022-12-30</td>\n",
       "      <td>What’s Gone at Twitter? A Data Center, Janitor...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2518 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Date                                      NewsHeadlines\n",
       "0     2013-01-02  New York’s Office Builders Raise Their Online ...\n",
       "1     2013-01-03  A Gigantic Sigh of Relief as Tax Uncertainty E...\n",
       "2     2013-01-04  Rig Owner Will Settle With U.S. in Gulf Spill ...\n",
       "3     2013-01-07  Europe Likely to Be Harder on Google Over Sear...\n",
       "4     2013-01-08  Avis and Hertz Acquisitions Raise Questions Ov...\n",
       "...          ...                                                ...\n",
       "2513  2022-12-23  Twitter Is Said to Have Struggled Over Reveali...\n",
       "2514  2022-12-27  Alameda Executive Says She Is ‘Truly Sorry’ fo...\n",
       "2515  2022-12-28  Thousands Will Live Here One Day (as Long as T...\n",
       "2516  2022-12-29  10 Months Into Job, Southwest’s C.E.O. Faces a...\n",
       "2517  2022-12-30  What’s Gone at Twitter? A Data Center, Janitor...\n",
       "\n",
       "[2518 rows x 2 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Building the training Dataset\n",
    "# Sorting the news articles according to Market days\n",
    "# Group articles of the same day together, a day is defined as the time between market close and next market open\n",
    "\n",
    "TrainingSet = pd.DataFrame({'Date' : [], 'NewsHeadlines' : []})\n",
    "data = ''\n",
    "for i in range(delta.days + 1):\n",
    "    day = specific_date_time + timedelta(days=i)\n",
    "    day = str(day)\n",
    "    \n",
    "    if day in tradingDays['Date'].values:\n",
    "        data += getArticles(day, 'b')\n",
    "        new_row = {'Date': day, 'NewsHeadlines': data}\n",
    "        TrainingSet = TrainingSet.append(new_row, ignore_index=True)\n",
    "        data = ''\n",
    "        data += getArticles(day,'a')\n",
    "    else:\n",
    "        data += getArticles(day,'s')\n",
    "        \n",
    "TrainingSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "426d6ce7",
   "metadata": {
    "id": "426d6ce7",
    "outputId": "0918823f-8ae1-4862-9ba5-f22017b4569e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# NLP Method: TF-Idf\n",
    "# Tokenise every element in the text\n",
    "text_tokens = []\n",
    "tokenizer = RegexpTokenizer(r'\\w+') # to not include punctuation\n",
    "for text in TrainingSet['NewsHeadlines']:\n",
    "    text_tokens.append(tokenizer.tokenize(text))\n",
    "\n",
    "# Case-Folding and Stop Word removal and Stemming for every word\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words.extend(['said', 'b', '000']) # add other stopwords\n",
    "ps = PorterStemmer()\n",
    "\n",
    "for i in range(len(text_tokens)):\n",
    "    text_tokens[i] = [ps.stem(word.casefold()) for word in text_tokens[i] if word.casefold() not in stop_words]\n",
    "\n",
    "# Find vocabulary - unique terms\n",
    "vocab = list(set(word for text in text_tokens for word in text))\n",
    "\n",
    "# Get count for every term in every doc and normalise this term frequency by dividing with max term frequency in specific day's text\n",
    "rows = len(text_tokens) # articles\n",
    "col = len(vocab) # words\n",
    "tf = np.zeros((rows, col))\n",
    "for i in range(len(text_tokens)):\n",
    "    max_tf = max(text_tokens[i].count(word) for word in text_tokens[i])\n",
    "    for j in range(col):\n",
    "        tf[i][j] = text_tokens[i].count(vocab[j]) / max_tf\n",
    "\n",
    "# Find the DF for every term in vocabulary and if DF is less than n, remove term\n",
    "N = len(tf)\n",
    "df = np.zeros(col)\n",
    "to_remove = []\n",
    "for j in range(col):\n",
    "    df[j] = sum(tf[i][j] > 0 for i in range(rows))\n",
    "    if df[j] < 200:\n",
    "        to_remove.append(j)\n",
    "\n",
    "for j in reversed(to_remove):\n",
    "    vocab.pop(j)\n",
    "    tf = np.delete(tf, j, axis=1)\n",
    "    df = np.delete(df, j)\n",
    "        \n",
    "# Calculate IDF using last n days\n",
    "dayLookBack = 5 # looking back 5 trading days\n",
    "idf = np.zeros((rows, len(vocab)))\n",
    "\n",
    "for row, col in enumerate(idf):\n",
    "    for idx in range(0, len(col)):\n",
    "        df_lookback = sum(tf[i][idx] > 0 for i in range(max(0, row - dayLookBack), row+1))\n",
    "        if df_lookback > 0:\n",
    "            idf[row][idx] = np.log(dayLookBack / df_lookback) \n",
    "            # else leave it 0\n",
    "\n",
    "# Calculate TF-IDF\n",
    "tf_idf = np.multiply(tf, idf)\n",
    "# normalize the TF-IDF matrix using L2 normalization\n",
    "tf_idf = normalize(tf_idf, norm='l2', axis=1)\n",
    "\n",
    "# Print the TF-IDF matrix\n",
    "print(tf_idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80782567",
   "metadata": {
    "id": "80782567"
   },
   "outputs": [],
   "source": [
    "# NLP Method: GloVe (Global Vectors for Word Representation) \n",
    "\n",
    "# Load the small English model in spaCy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Find the length of the longest vector (every vector must have same length)\n",
    "max_len = 0\n",
    "for text in TrainingSet['NewsHeadlines']:\n",
    "    doc = nlp(text)\n",
    "    if len(doc) > max_len:\n",
    "        max_len = len(doc)\n",
    "\n",
    "# Create an empty list to store the GloVe vectors\n",
    "glove_vectors = []\n",
    "\n",
    "# Iterate over each text in the NewsHeadlines column\n",
    "for text in TrainingSet['NewsHeadlines']:\n",
    "    # Tokenize the text using spaCy\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    # Get the GloVe vector for each token\n",
    "    token_vectors = [token.vector for token in doc]\n",
    "    \n",
    "    # Pad the vectors to the maximum length\n",
    "    if len(token_vectors) < max_len:\n",
    "        token_vectors += [np.zeros_like(token_vectors[0])] * (max_len - len(token_vectors))\n",
    "    \n",
    "    # Get the average GloVe vector for the text\n",
    "    text_vector = np.mean(token_vectors, axis=0)\n",
    "    \n",
    "    # Add the text vector to the list of vectors\n",
    "    glove_vectors.append(text_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "067e62fb",
   "metadata": {
    "id": "067e62fb"
   },
   "outputs": [],
   "source": [
    "# Alternative to NLP representations: getting sentiment score using VADER (Valence Aware Dictionary and sEntiment Reasoner),\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "sentiment_scores = []\n",
    "# Get the sentiment values for each headline\n",
    "for text in TrainingSet['NewsHeadlines']:\n",
    "    sentiment = sia.polarity_scores(text)\n",
    "    sentiment_scores.append(sentiment['compound'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa8b8ae6",
   "metadata": {
    "id": "fa8b8ae6"
   },
   "outputs": [],
   "source": [
    "# Vector representation and Sentiment scores for news article\n",
    "News_dataset = pd.DataFrame({\n",
    "    'Date': TrainingSet['Date'].astype(str),\n",
    "    'tf_idf': tf_idf.tolist(),\n",
    "    'glove' : glove_vectors,\n",
    "    'sentiment scores': sentiment_scores\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce547336",
   "metadata": {
    "id": "ce547336"
   },
   "outputs": [],
   "source": [
    "# Save info\n",
    "# Save News_dataset\n",
    "# News_dataset.to_feather('OldTrain.feather')\n",
    "# Save Vocab\n",
    "# with open('vocab.pkl', 'wb') as f:\n",
    "#    pickle.dump(vocab, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6332302e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 380
    },
    "id": "6332302e",
    "outputId": "e4fd8498-fdc3-4139-eab7-5ab6833a80cf"
   },
   "outputs": [],
   "source": [
    "# load the news data (** cell to load all the preprocessed news data)\n",
    "# Load News_dataset\n",
    "News_dataset = pd.read_feather(\"preprocessed_datasets/Train_2020-2023.feather\")\n",
    "\n",
    "# Load Vocab\n",
    "with open('preprocessed_datasets/vocab_df_3.pkl', 'rb') as f:\n",
    "    vocab_df_3 = pickle.load(f)\n",
    "\n",
    "with open('preprocessed_datasets/vocab_df_50.pkl', 'rb') as f:\n",
    "    vocab_df_50 = pickle.load(f)\n",
    "\n",
    "with open('preprocessed_datasets/vocab_df_100.pkl', 'rb') as f:\n",
    "    vocab_df_100 = pickle.load(f)\n",
    "\n",
    "with open('preprocessed_datasets/vocab_df_200.pkl', 'rb') as f:\n",
    "    vocab_df_200 = pickle.load(f)\n",
    "\n",
    "with open('preprocessed_datasets/vocab_sentiment_dict.pkl', 'rb') as f:\n",
    "    vocab_sentiment_dict = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "abhpfDelSbGu",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 832
    },
    "id": "abhpfDelSbGu",
    "outputId": "76e73097-98d2-4cff-b85d-c1dbf8cbcedb"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>tf_idf</th>\n",
       "      <th>glove</th>\n",
       "      <th>sentiment scores</th>\n",
       "      <th>tf_idf_common_50</th>\n",
       "      <th>tf_idf_common_100</th>\n",
       "      <th>tf_idf_custom_sentiment</th>\n",
       "      <th>tf_idf_common_200</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2013-01-03</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0065032337, -0.026966467, -0.010776335, 0.0...</td>\n",
       "      <td>0.9941</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.06365168830912431,...</td>\n",
       "      <td>[0.0, 0.0, 0.06754302064687229, 0.0, 0.0, 0.0,...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.07219226694801421,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2013-01-04</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.008706812, -0.031403296, -0.021160785, 0.00...</td>\n",
       "      <td>0.9556</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.035472046521935446, 0.062305...</td>\n",
       "      <td>[0.0, 0.0558712519146814, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2013-01-07</td>\n",
       "      <td>[0.0, 0.03853083814037951, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "      <td>[0.0030595479, -0.050828382, -0.03941388, 0.04...</td>\n",
       "      <td>0.9881</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.023406448620545438, 0.0...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.04431067049561818, 0.0, 0.0, 0.02...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2013-01-08</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0090215495, -0.017810782, -0.011544912, 0.0...</td>\n",
       "      <td>0.8466</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.07137878093524182, 0.0, 0.0, 0.0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2013-01-09</td>\n",
       "      <td>[0.0, 0.035378497414749396, 0.0, 0.0, 0.0, 0.0...</td>\n",
       "      <td>[0.0021759986, -0.045274474, -0.022185955, 0.0...</td>\n",
       "      <td>-0.9296</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.06773414184281647, 0.0,...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.023177936839543795, 0.0...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.08393975665850119, 0.0, 0.0, 0.083939756658...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Date                                             tf_idf  \\\n",
       "0  2013-01-03  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "1  2013-01-04  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "2  2013-01-07  [0.0, 0.03853083814037951, 0.0, 0.0, 0.0, 0.0,...   \n",
       "3  2013-01-08  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "4  2013-01-09  [0.0, 0.035378497414749396, 0.0, 0.0, 0.0, 0.0...   \n",
       "\n",
       "                                               glove  sentiment scores  \\\n",
       "0  [0.0065032337, -0.026966467, -0.010776335, 0.0...            0.9941   \n",
       "1  [0.008706812, -0.031403296, -0.021160785, 0.00...            0.9556   \n",
       "2  [0.0030595479, -0.050828382, -0.03941388, 0.04...            0.9881   \n",
       "3  [0.0090215495, -0.017810782, -0.011544912, 0.0...            0.8466   \n",
       "4  [0.0021759986, -0.045274474, -0.022185955, 0.0...           -0.9296   \n",
       "\n",
       "                                    tf_idf_common_50  \\\n",
       "0  [0.0, 0.0, 0.0, 0.0, 0.0, 0.06365168830912431,...   \n",
       "1  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "2  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "3  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "4  [0.0, 0.0, 0.0, 0.0, 0.06773414184281647, 0.0,...   \n",
       "\n",
       "                                   tf_idf_common_100  \\\n",
       "0  [0.0, 0.0, 0.06754302064687229, 0.0, 0.0, 0.0,...   \n",
       "1  [0.0, 0.0, 0.0, 0.035472046521935446, 0.062305...   \n",
       "2  [0.0, 0.0, 0.0, 0.0, 0.023406448620545438, 0.0...   \n",
       "3  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "4  [0.0, 0.0, 0.0, 0.0, 0.023177936839543795, 0.0...   \n",
       "\n",
       "                             tf_idf_custom_sentiment  \\\n",
       "0  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "1  [0.0, 0.0558712519146814, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "2  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "3  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "4  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "\n",
       "                                   tf_idf_common_200  \n",
       "0  [0.0, 0.0, 0.0, 0.0, 0.0, 0.07219226694801421,...  \n",
       "1  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "2  [0.0, 0.0, 0.04431067049561818, 0.0, 0.0, 0.02...  \n",
       "3  [0.0, 0.0, 0.07137878093524182, 0.0, 0.0, 0.0,...  \n",
       "4  [0.08393975665850119, 0.0, 0.0, 0.083939756658...  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove the first day of news data since we do not need it (we always predict from tommo open -> close)\n",
    "News_dataset = News_dataset.drop(index=0)\n",
    "# arrange index\n",
    "News_dataset = News_dataset.reset_index()\n",
    "del News_dataset['index']\n",
    "News_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "befbaffa",
   "metadata": {
    "id": "befbaffa"
   },
   "outputs": [],
   "source": [
    "# Choose NLP representation\n",
    "# News_dataset['News Vector'] = News_dataset['tf_idf'] # df > 3\n",
    "# News_dataset['News Vector'] = News_dataset['glove']\n",
    "# News_dataset['News Vector'] = News_dataset['tf_idf_common_50']\n",
    "# News_dataset['News Vector'] = News_dataset['tf_idf_common_100']\n",
    "News_dataset['News Vector'] = News_dataset['tf_idf_common_200']\n",
    "# News_dataset['News Vector'] = News_dataset['tf_idf_custom_sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "122f9c6b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "122f9c6b",
    "outputId": "60d2b06e-a977-4860-94cf-af40975f8556"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The top 10 terms are ['reserv', 'watch', 'feder', 'dealbook', 'increas', 'panel', 'island', 'though', 'huge', 'appear']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['reserv',\n",
       " 'watch',\n",
       " 'feder',\n",
       " 'dealbook',\n",
       " 'increas',\n",
       " 'panel',\n",
       " 'island',\n",
       " 'though',\n",
       " 'huge',\n",
       " 'appear']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def Get_Keywords(date, vocab):\n",
    "    # get news vector for the given date\n",
    "    vector = np.array(News_dataset.loc[News_dataset['Date'] == date]['News Vector'].iloc[0].copy())\n",
    "    top_terms = []\n",
    "    i = 0\n",
    "    # get top 10 terms from news vector\n",
    "    while len(top_terms) < 10:\n",
    "        max_index = vector.argmax()\n",
    "        if vector[max_index] == 0:\n",
    "            return top_terms\n",
    "        term = vocab[max_index]\n",
    "        if term not in top_terms:\n",
    "            top_terms.append(term)\n",
    "        vector[max_index] = -1\n",
    "        i = i + 1\n",
    "    print('The top 10 terms are', top_terms)\n",
    "    return top_terms\n",
    "\n",
    "Get_Keywords('2022-12-01', vocab_df_200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "907fbcaf",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 165
    },
    "id": "907fbcaf",
    "outputId": "4cd238d0-c79e-4f14-c34f-1b402ca48c3f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sam Bankman-Fried Blames ‘Huge Management Failures’ for FTX Collapse Mr. Bankman-Fried spoke at The New York Times’s DealBook conference, in his first public appearance since his crypto exchange imploded. FTX’s Sister Firm, Alameda Research, Was Central to Collapse The relationship between Alameda and FTX — and how the two propped each other up — is coming under scrutiny as prosecutors and regulators investigate the exchange’s collapse. Watch live as Sam Bankman-Fried speaks with DealBook. Sam Bankman-Fried, the founder of the collapsed cryptocurrency exchange FTX, is set to speak to Andrew Ross Sorkin at 5 p.m. Eastern time at the DealBook Summit. You can watch it here and follow along with live analysis by our reporters. Epstein Estate Agrees to Pay More Than $105 Million to U.S. Virgin Islands The estate of the disgraced financier Jeffrey Epstein will repay tax benefits and half the proceeds from the sale of an island he owned. Mark Zuckerberg defends company’s push into the metaverse, where he appeared with a bout of hiccups. The company has poured billions into the construction of the metaverse, which has been slow to take off. Powell Says Fed Could Slow Rate Increases at Next Meeting The Federal Reserve chair said the central bank would keep lifting rates to control price increases, and played down a recent cooling in inflation. Andy Jassy says Amazon will allow the antisemitic film that Kyrie Irving tweeted to remain on sale for now. He said that the company has a panel that reviews content that might be objectionable, but that cases were not always “straightforward.” Layoffs Hit CNN as Cost-Cutting Pressure Mounts Chris Licht, the network’s chairman, said the company will notify affected employees this week. Some Rail Workers, Seeking Sick Days, Say Biden Betrayed Them The request for Congress to impose contract terms that several unions had rejected rankled rank-and-file members who had rallied behind the president. Job Openings Ease, but Layoffs Are Little Changed Government data for October shows the labor market is still strong, though cooling slightly. President Zelensky of Ukraine rebukes Elon Musk’s peace proposal. Speaking at the DealBook summit, the Ukrainian leader said the billionaire would do well to fully understand the situation before making pronouncements about it. Stocks Jump as Fed’s Powell Signals a Slowdown in Rate Increases In his final public remarks before a crucial Federal Reserve meeting, the central bank’s chair said the time to moderate rate increases could come as soon as December.Consumers Kept Up Their Spending in October, Even as Prices Rose Consumption climbed and personal income rose, even after accounting for inflation, new data from the Commerce Department showed. The Fed’s Preferred Inflation Gauge Slowed in October A measure of inflation that the Federal Reserve watches most closely is showing signs of moderation, though price gains still remain too fast for comfort. California Panel Sizes Up Reparations for Black Citizens The state is undertaking the nation’s most ambitious effort so far to compensate for the economic legacy of slavery and racism. How the Collapse of Sam Bankman-Fried’s Crypto Empire Has Disrupted A.I. Mr. Bankman-Fried and his colleagues spent more than $530 million to battle what they saw as the dangers of artificial intelligence. Now those efforts are reeling. Transcript of Sam Bankman-Fried’s Interview at the DealBook Summit In a discussion with Andrew Ross Sorkin of The New York Times, Sam Bankman-Fried blamed “huge management failures” and sloppy accounting for the collapse of the FTX cryptocurrency exchange. Who Made Your World Cup Jersey? Garment workers in Myanmar earn less than $3 a day to produce soccer apparel for Adidas. Some say they were fired after asking factory owners for a raise. Elon Musk Says ‘Misunderstanding’ With Apple Is Resolved Mr. Musk, who had said the company was trying to sabotage Twitter, met with Apple’s chief executive, Tim Cook, on Wednesday. Ignoring Legal Advice, Sam Bankman-Fried Speaks on FTX The fallen founder of the cryptocurrency empire said it was his “duty” to talk about the exchange’s collapse. Sam Bankman-Fried’s Phone Call to His Parents: ‘There Might Be a Liquidity Issue’ The onetime crypto mogul also says he’s running out of funds, holed up in the Bahamas.\n"
     ]
    }
   ],
   "source": [
    "print('\\n'.join(TrainingSet[TrainingSet['Date'] == '2022-12-01']['NewsHeadlines']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "4cc4954f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 235
    },
    "id": "4cc4954f",
    "outputId": "2173b041-6b44-47f2-d58a-d9b1201890ae"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>1056</th>\n",
       "      <th>1057</th>\n",
       "      <th>1058</th>\n",
       "      <th>1059</th>\n",
       "      <th>1060</th>\n",
       "      <th>1061</th>\n",
       "      <th>1062</th>\n",
       "      <th>1063</th>\n",
       "      <th>1064</th>\n",
       "      <th>1065</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.072192</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.072192</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.038710</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.067994</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.044311</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.025227</td>\n",
       "      <td>0.044311</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.044311</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.014064</td>\n",
       "      <td>0.025227</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.044311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.071379</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.125375</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.08394</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.08394</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.08394</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1066 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0    1         2        3    4         5         6    7    8  \\\n",
       "0  0.00000  0.0  0.000000  0.00000  0.0  0.072192  0.000000  0.0  0.0   \n",
       "1  0.00000  0.0  0.000000  0.00000  0.0  0.000000  0.000000  0.0  0.0   \n",
       "2  0.00000  0.0  0.044311  0.00000  0.0  0.025227  0.044311  0.0  0.0   \n",
       "3  0.00000  0.0  0.071379  0.00000  0.0  0.000000  0.000000  0.0  0.0   \n",
       "4  0.08394  0.0  0.000000  0.08394  0.0  0.000000  0.000000  0.0  0.0   \n",
       "\n",
       "          9  ...      1056     1057  1058  1059      1060      1061  1062  \\\n",
       "0  0.000000  ...  0.000000  0.00000   0.0   0.0  0.072192  0.000000   0.0   \n",
       "1  0.000000  ...  0.000000  0.00000   0.0   0.0  0.038710  0.000000   0.0   \n",
       "2  0.044311  ...  0.000000  0.00000   0.0   0.0  0.014064  0.025227   0.0   \n",
       "3  0.000000  ...  0.125375  0.00000   0.0   0.0  0.000000  0.000000   0.0   \n",
       "4  0.000000  ...  0.000000  0.08394   0.0   0.0  0.000000  0.000000   0.0   \n",
       "\n",
       "       1063  1064      1065  \n",
       "0  0.000000   0.0  0.000000  \n",
       "1  0.067994   0.0  0.000000  \n",
       "2  0.000000   0.0  0.044311  \n",
       "3  0.000000   0.0  0.000000  \n",
       "4  0.000000   0.0  0.000000  \n",
       "\n",
       "[5 rows x 1066 columns]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NewsVectors = pd.DataFrame(News_dataset['News Vector'].values.tolist())\n",
    "NewsVectors.columns = NewsVectors.columns.astype(str)\n",
    "NewsVectors.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "df202dda",
   "metadata": {
    "id": "df202dda"
   },
   "outputs": [],
   "source": [
    "# combine NewsVectors with technical indicators\n",
    "stock_dataset = normalised_dataset.join(NewsVectors).drop(columns=['Open','Adj Close'])\n",
    "stock_dataset['Today return'] = stock_dataset['Today return'].apply(lambda x: 0 if x < 0 else 1) # convert column to either 0 or 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "80206e54",
   "metadata": {
    "id": "80206e54"
   },
   "outputs": [],
   "source": [
    "# Options\n",
    "# 1) Using both news and technical indicators\n",
    "training_dataset = stock_dataset.drop(columns=['High','Low','Close','Volume','Today return'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "RVa2WVwoYEJE",
   "metadata": {
    "id": "RVa2WVwoYEJE"
   },
   "outputs": [],
   "source": [
    "# 2) Using just technical indicators\n",
    "training_dataset = normalised_dataset.drop(columns=['High','Low','Close','Adj Close', 'Open','Volume','Today return'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "14cfc55c",
   "metadata": {
    "id": "14cfc55c"
   },
   "outputs": [],
   "source": [
    "# 3) Using just news vector\n",
    "training_dataset = NewsVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f7e7c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4) Add sentiment scores\n",
    "training_dataset['sentiment scores'] = News_dataset['sentiment scores']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "e54a37f7",
   "metadata": {
    "id": "e54a37f7"
   },
   "outputs": [],
   "source": [
    "# Get the date again\n",
    "training_dataset['Date'] = pd.to_datetime(News_dataset['Date']) # we are predicting from this trading day's open -> (example close if Tommo_Direction)\n",
    "\n",
    "# Get label also\n",
    "training_dataset['Label'] = stock_data['Tommo_Direction'] # if we predicting tommo direction\n",
    "#training_dataset['Label'] = stock_data['3Day_Direction'] # if we predicting next 3 Day Direction\n",
    "#training_dataset['Label'] = stock_data['5Day_Direction'] # if we predicting 5 Day Direction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "5CnwMWs0OVKf",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "id": "5CnwMWs0OVKf",
    "outputId": "21c9ebb2-d076-4ab8-8da8-3ecbac3c56c6"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>1058</th>\n",
       "      <th>1059</th>\n",
       "      <th>1060</th>\n",
       "      <th>1061</th>\n",
       "      <th>1062</th>\n",
       "      <th>1063</th>\n",
       "      <th>1064</th>\n",
       "      <th>1065</th>\n",
       "      <th>Date</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.072192</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.072192</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2013-01-03</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.038710</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.067994</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2013-01-04</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.044311</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.025227</td>\n",
       "      <td>0.044311</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.044311</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.014064</td>\n",
       "      <td>0.025227</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.044311</td>\n",
       "      <td>2013-01-07</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.071379</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2013-01-08</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.08394</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.08394</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2013-01-09</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2512</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.052939</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2022-12-23</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2513</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.081422</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.081422</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.092710</td>\n",
       "      <td>2022-12-27</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2514</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.109728</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2022-12-28</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2515</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.081818</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2022-12-29</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2516</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.073561</td>\n",
       "      <td>2022-12-30</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2517 rows × 1068 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            0         1         2        3    4         5         6    7    8  \\\n",
       "0     0.00000  0.000000  0.000000  0.00000  0.0  0.072192  0.000000  0.0  0.0   \n",
       "1     0.00000  0.000000  0.000000  0.00000  0.0  0.000000  0.000000  0.0  0.0   \n",
       "2     0.00000  0.000000  0.044311  0.00000  0.0  0.025227  0.044311  0.0  0.0   \n",
       "3     0.00000  0.000000  0.071379  0.00000  0.0  0.000000  0.000000  0.0  0.0   \n",
       "4     0.08394  0.000000  0.000000  0.08394  0.0  0.000000  0.000000  0.0  0.0   \n",
       "...       ...       ...       ...      ...  ...       ...       ...  ...  ...   \n",
       "2512  0.00000  0.000000  0.000000  0.00000  0.0  0.000000  0.000000  0.0  0.0   \n",
       "2513  0.00000  0.081422  0.000000  0.00000  0.0  0.081422  0.000000  0.0  0.0   \n",
       "2514  0.00000  0.000000  0.000000  0.00000  0.0  0.109728  0.000000  0.0  0.0   \n",
       "2515  0.00000  0.081818  0.000000  0.00000  0.0  0.000000  0.000000  0.0  0.0   \n",
       "2516  0.00000  0.000000  0.000000  0.00000  0.0  0.000000  0.000000  0.0  0.0   \n",
       "\n",
       "             9  ...      1058  1059      1060      1061  1062      1063  1064  \\\n",
       "0     0.000000  ...  0.000000   0.0  0.072192  0.000000   0.0  0.000000   0.0   \n",
       "1     0.000000  ...  0.000000   0.0  0.038710  0.000000   0.0  0.067994   0.0   \n",
       "2     0.044311  ...  0.000000   0.0  0.014064  0.025227   0.0  0.000000   0.0   \n",
       "3     0.000000  ...  0.000000   0.0  0.000000  0.000000   0.0  0.000000   0.0   \n",
       "4     0.000000  ...  0.000000   0.0  0.000000  0.000000   0.0  0.000000   0.0   \n",
       "...        ...  ...       ...   ...       ...       ...   ...       ...   ...   \n",
       "2512  0.000000  ...  0.052939   0.0  0.000000  0.000000   0.0  0.000000   0.0   \n",
       "2513  0.000000  ...  0.000000   0.0  0.000000  0.000000   0.0  0.000000   0.0   \n",
       "2514  0.000000  ...  0.000000   0.0  0.000000  0.000000   0.0  0.000000   0.0   \n",
       "2515  0.000000  ...  0.000000   0.0  0.000000  0.000000   0.0  0.000000   0.0   \n",
       "2516  0.000000  ...  0.000000   0.0  0.000000  0.000000   0.0  0.000000   0.0   \n",
       "\n",
       "          1065       Date  Label  \n",
       "0     0.000000 2013-01-03    0.0  \n",
       "1     0.000000 2013-01-04    1.0  \n",
       "2     0.044311 2013-01-07    0.0  \n",
       "3     0.000000 2013-01-08    0.0  \n",
       "4     0.000000 2013-01-09    1.0  \n",
       "...        ...        ...    ...  \n",
       "2512  0.000000 2022-12-23    1.0  \n",
       "2513  0.092710 2022-12-27    0.0  \n",
       "2514  0.000000 2022-12-28    0.0  \n",
       "2515  0.000000 2022-12-29    1.0  \n",
       "2516  0.073561 2022-12-30    1.0  \n",
       "\n",
       "[2517 rows x 1068 columns]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "lkUCp-cveszc",
   "metadata": {
    "id": "lkUCp-cveszc"
   },
   "outputs": [],
   "source": [
    "# Split this training set into train (and validation) and test sets according to date\n",
    "Train_Test_Sets = []\n",
    "# Create a list of the train-test split dates\n",
    "train_test_splits = [\n",
    "          ('2013-01-01', '2019-12-31', '2020-01-01', '2020-12-31'),\n",
    "          ('2013-01-01', '2020-12-31', '2021-01-01', '2021-12-31'),\n",
    "          ('2013-01-01', '2021-12-31', '2022-01-01', '2022-12-31'),]\n",
    "\n",
    "# Loop over the list of splits and create the corresponding train and test sets\n",
    "for i, (train_start, train_end, test_start, test_end) in enumerate(train_test_splits):\n",
    "    train_set = training_dataset[(training_dataset['Date'] >= train_start) & (training_dataset['Date'] <= train_end)].drop(columns=['Date'])\n",
    "    test_set = training_dataset[(training_dataset['Date'] >= test_start) & (training_dataset['Date'] <= test_end)].drop(columns=['Date'])\n",
    "    Train_Test_Sets.append((train_set,test_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "dzWpSoWOzuDm",
   "metadata": {
    "id": "dzWpSoWOzuDm"
   },
   "outputs": [],
   "source": [
    "# Tensors containing all train data to get hyper-parameters for models\n",
    "x_Train_set = Train_Test_Sets[0][0].drop(columns=['Label'])\n",
    "y_Train_set = Train_Test_Sets[0][0]['Label']\n",
    "X_tensor = torch.tensor(x_Train_set.values, dtype=torch.float32, device=device)\n",
    "y_tensor = torch.tensor(y_Train_set.to_numpy(), dtype=torch.float32, device=device)\n",
    "input_size = len(X_tensor[0])\n",
    "\n",
    "# use k-fold validation to check model accuracy\n",
    "n_splits = 10\n",
    "tscv = TimeSeriesSplit(n_splits=n_splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "5fN9kNh6G7Ks",
   "metadata": {
    "id": "5fN9kNh6G7Ks"
   },
   "outputs": [],
   "source": [
    "# Artificial Neural Network\n",
    "class ANN(torch.nn.Module):\n",
    "    def __init__(self, input_layer, hidden_layer, dropout):\n",
    "        super().__init__()\n",
    "\n",
    "        self.Leakyrelu = torch.nn.LeakyReLU(0.1)\n",
    "        self.dropout = torch.nn.Dropout(dropout)\n",
    "\n",
    "        # Add dense layers\n",
    "        self.fc_0 = torch.nn.Linear(input_layer, hidden_layer) # input layer\n",
    "        self.fc_1 = torch.nn.Linear(hidden_layer, hidden_layer)\n",
    "        self.fc_2 = torch.nn.Linear(hidden_layer, hidden_layer)\n",
    "        self.fc_3 = torch.nn.Linear(hidden_layer, 1) # output layer\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        # Perform the fc layers\n",
    "        \n",
    "        x = self.Leakyrelu(self.fc_0(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.Leakyrelu(self.fc_1(x))\n",
    "        x = self.Leakyrelu(self.fc_2(x))\n",
    "        x = self.fc_3(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "# function to help train model on data split\n",
    "def train_model_on_split_ann(X_train, y_train, X_val, y_val, hyperparams):\n",
    "    # build the NN with these given params\n",
    "    (learning_rate, epoch, dropout, hidden_layer) = hyperparams\n",
    "    # Instantiate the model\n",
    "    model_ANN = ANN(len(X_train[0]), hidden_layer, dropout)\n",
    "    model_ANN = model_ANN.to(device)\n",
    "    # Define the optimizer\n",
    "    optimiser = torch.optim.Adam(model_ANN.parameters(), lr=learning_rate)\n",
    "\n",
    "    # train the split\n",
    "    best_val_acc = 0.00\n",
    "    for step in range(1, epoch+1):\n",
    "        output = model_ANN(X_train)\n",
    "        error = torch.nn.functional.binary_cross_entropy_with_logits(output, y_train.unsqueeze(1))\n",
    "\n",
    "        # Backward pass\n",
    "        optimiser.zero_grad()\n",
    "        error.backward()\n",
    "        optimiser.step()\n",
    "\n",
    "        # Get Accuracy on validation set (if training acc > 0.5)\n",
    "        with torch.no_grad():\n",
    "            # validation set\n",
    "            predictions = torch.round(torch.sigmoid(model_ANN(X_train)))[:, 0].detach().cpu().numpy()\n",
    "            if not np.isnan(predictions).any():\n",
    "                train_acc = accuracy_score(y_train.cpu(), predictions)\n",
    "            else:\n",
    "                train_acc = 0\n",
    "\n",
    "            if train_acc > 0.5:\n",
    "                #  get validation accuracy\n",
    "                predictions = torch.round(torch.sigmoid(model_ANN(X_val)))[:, 0].detach().cpu().numpy()\n",
    "                val_acc = accuracy_score(y_val.cpu(), predictions)\n",
    "                if val_acc > best_val_acc:\n",
    "                    best_val_acc = val_acc\n",
    "                # if train acc is above 99% end training\n",
    "                if train_acc > 0.99:\n",
    "                    return best_val_acc\n",
    "    return best_val_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "wqNArrHC7ilJ",
   "metadata": {
    "id": "wqNArrHC7ilJ"
   },
   "outputs": [],
   "source": [
    "# Gated Recurrent Unit (GRU)\n",
    "class GRU_Neural_Network(torch.nn.Module):\n",
    "    def __init__(self, input_layer, hidden_layer_1, hidden_layer_2, dropout, num_layers):\n",
    "        super().__init__()\n",
    "\n",
    "        self.Leakyrelu = torch.nn.LeakyReLU(0.1)\n",
    "        self.dropout_1 = torch.nn.Dropout(dropout)\n",
    "\n",
    "        # Add a dense layer \n",
    "        self.fc_0 = torch.nn.Linear(input_layer, hidden_layer_1)\n",
    "\n",
    "        # GRU layers\n",
    "        self.gru_1 = torch.nn.GRU(hidden_layer_1, hidden_layer_2, num_layers, batch_first=True)\n",
    "\n",
    "        # Add a dense layer\n",
    "        self.fc_1 = torch.nn.Linear(hidden_layer_2, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Pass to the fc layer\n",
    "        x = self.Leakyrelu(self.fc_0(x))\n",
    "        x = self.dropout_1(x)\n",
    "        # Pass to the gru cell\n",
    "        x , _ = self.gru_1(x)\n",
    "        x = x[:, -1, :]\n",
    "        x = self.fc_1(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "XeQl8FQp72EA",
   "metadata": {
    "id": "XeQl8FQp72EA"
   },
   "outputs": [],
   "source": [
    "class LSTM_Neural_Network(torch.nn.Module):\n",
    "    def __init__(self, input_layer, hidden_layer_1, hidden_layer_2, dropout, num_layers):\n",
    "        super().__init__()\n",
    "\n",
    "        self.Leakyrelu = torch.nn.LeakyReLU(0.1)\n",
    "        self.dropout_1 = torch.nn.Dropout(dropout)\n",
    "\n",
    "        # Add a dense layer \n",
    "        self.fc_0 = torch.nn.Linear(input_layer, hidden_layer_1)\n",
    "\n",
    "        # LSTM layers\n",
    "        self.lstm_1 = torch.nn.LSTM(hidden_layer_1, hidden_layer_2, num_layers, batch_first=True)\n",
    "\n",
    "        # Add a dense layer\n",
    "        self.fc_1 = torch.nn.Linear(hidden_layer_2, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Pass to the fc layer\n",
    "        x = self.Leakyrelu(self.fc_0(x))\n",
    "        x = self.dropout_1(x)\n",
    "        # Pass to the lstm cell\n",
    "        x , _ = self.lstm_1(x)\n",
    "        x = x[:, -1, :]\n",
    "        x = self.fc_1(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "jQCF9mYi7gcQ",
   "metadata": {
    "id": "jQCF9mYi7gcQ"
   },
   "outputs": [],
   "source": [
    "# function to train model on data split and return best validation accuracy\n",
    "def train_model_on_split_gru_or_lstm(train_dataloader, val_dataloader, hyperparams, input_size, mode):\n",
    "    # get all train data into inputs and labels tensors\n",
    "    train_inputs, train_labels = next(iter(train_dataloader))\n",
    "    for inputs, labels in train_dataloader:\n",
    "        train_inputs = torch.cat([train_inputs, inputs], dim=0)\n",
    "        train_labels = torch.cat([train_labels, labels], dim=0)\n",
    "\n",
    "    # get all validation data into inputs and labels tensors\n",
    "    val_inputs, val_labels = next(iter(val_dataloader))\n",
    "    for inputs, labels in val_dataloader:\n",
    "        val_inputs = torch.cat([val_inputs, inputs], dim=0)\n",
    "        val_labels = torch.cat([val_labels, labels], dim=0)\n",
    "\n",
    "    # build the NN with these given params\n",
    "    (learning_rate, epoch, dropout, hidden_layer_1, hidden_layer_2, num_layers, _, _) = hyperparams\n",
    "\n",
    "    # Instantiate the model\n",
    "    if mode == 'GRU':\n",
    "        model = GRU_Neural_Network(input_size, hidden_layer_1, hidden_layer_2, dropout, num_layers)\n",
    "        model = model.to(device)\n",
    "    if mode == 'LSTM':\n",
    "        model = LSTM_Neural_Network(input_size, hidden_layer_1, hidden_layer_2, dropout, num_layers)\n",
    "        model = model.to(device)\n",
    "\n",
    "\n",
    "    # Define the optimiser\n",
    "    optimiser = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    best_val_acc = 0.00\n",
    "\n",
    "    # train on the split\n",
    "    # train the model\n",
    "    for step in range(epoch):\n",
    "        model.train()\n",
    "        # train on batches\n",
    "        for inputs, labels in train_dataloader:\n",
    "            optimiser.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = torch.nn.functional.binary_cross_entropy_with_logits(outputs, labels.unsqueeze(1))\n",
    "            loss.backward()\n",
    "            optimiser.step()\n",
    "        # check accuracy on train set and validation set\n",
    "\n",
    "        # Get Accuracy on validation set (if training acc > 0.5)\n",
    "        with torch.no_grad():\n",
    "            # validation set\n",
    "            predictions = torch.round(torch.sigmoid(model(train_inputs)))[:, 0].detach().cpu().numpy()\n",
    "            if not np.isnan(predictions).any():\n",
    "                train_acc = accuracy_score(train_labels.cpu(), predictions)\n",
    "            else:\n",
    "                train_acc = 0\n",
    "\n",
    "            if train_acc > 0.5:\n",
    "                #  get validation accuracy\n",
    "                predictions = torch.round(torch.sigmoid(model(val_inputs)))[:, 0].detach().cpu().numpy()\n",
    "                val_acc = accuracy_score(val_labels.cpu(), predictions)\n",
    "                if val_acc > best_val_acc:\n",
    "                    best_val_acc = val_acc\n",
    "                # if train acc is above 99% end training\n",
    "                if train_acc > 0.99:\n",
    "                    return best_val_acc\n",
    "                  \n",
    "    return best_val_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ipOQCPhWWlAR",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ipOQCPhWWlAR",
    "outputId": "8181517c-20ec-4ffc-a8d6-4e7ce7fb44c1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Found better accuracy of 0.551875\n",
      "3 Found better accuracy of 0.580625\n",
      "8 Found better accuracy of 0.5831249999999999\n",
      "9 Found better accuracy of 0.583125\n",
      "14 Found better accuracy of 0.588125\n",
      "ANN CV average: 0.588125\n",
      "Best Params, For ANN:\n",
      "learning_rate: 1e-05\n",
      "epoch: 500\n",
      "dropout: 0.3\n",
      "hidden_layer_size: 750\n"
     ]
    }
   ],
   "source": [
    "# ANN\n",
    "# Chosing a suitable model : Using random search through parameters and Cross Validation\n",
    "\n",
    "# Random search through parameters combinations to find a suitable model\n",
    "learning_rate_set = [0.001, 0.0001, 0.00001]\n",
    "epoch_set = [500, 750, 1_000]\n",
    "dropout_set = [0.2, 0.3, 0.4]\n",
    "hidden_layer_size_set = [64, 128, 256, 400, 500, 750]\n",
    "\n",
    "already_generated = set ()\n",
    "best_overall_val_acc = 0.0\n",
    "best_hyperparams = None\n",
    "\n",
    "for count in range(1, 15+1):\n",
    "    found = True\n",
    "    while found:\n",
    "        learning_rate = random.choice(learning_rate_set)\n",
    "        epoch = random.choice(epoch_set)\n",
    "        dropout = random.choice(dropout_set)\n",
    "        hidden_layer_size = random.choice(hidden_layer_size_set)\n",
    "\n",
    "        hyperparams = (learning_rate, epoch, dropout, hidden_layer_size)\n",
    "        if hyperparams not in already_generated:\n",
    "            already_generated.add(hyperparams)\n",
    "            found = False\n",
    "    \n",
    "    # Training the model using k-fold validation\n",
    "    fold_val_accuracy = []\n",
    "    for fold, (train_index, val_index) in enumerate(tscv.split(X_tensor)):\n",
    "        X_train, X_val = X_tensor[train_index], X_tensor[val_index]\n",
    "        y_train, y_val = y_tensor[train_index], y_tensor[val_index]\n",
    "\n",
    "        score = train_model_on_split_ann(X_train, y_train, X_val, y_val, hyperparams)\n",
    "        fold_val_accuracy.append(score)\n",
    "\n",
    "        avg_val_acc = sum(fold_val_accuracy)/len(fold_val_accuracy)\n",
    "                        \n",
    "    # check if this param configuration had the best val accuracy\n",
    "    if avg_val_acc > best_overall_val_acc:\n",
    "        print(count,\"Found better accuracy of\",avg_val_acc)\n",
    "        best_overall_val_acc = avg_val_acc\n",
    "        best_hyperparams = hyperparams\n",
    "print(\"ANN CV average:\", best_overall_val_acc)\n",
    "# Best Params found for ANN\n",
    "(learning_rate_ann, epoch_ann, dropout_ann, hidden_layer_ann) = best_hyperparams\n",
    "print('Best Params, For ANN:')\n",
    "print('learning_rate:', learning_rate_ann)\n",
    "print('epoch:', epoch_ann)\n",
    "print('dropout:', dropout_ann) \n",
    "print('hidden_layer_size:', hidden_layer_ann)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "YL_JOZXsmAjX",
   "metadata": {
    "id": "YL_JOZXsmAjX"
   },
   "outputs": [],
   "source": [
    "(learning_rate_ann, epoch_ann, dropout_ann, hidden_layer_ann) = (0.0001, 750, 0.4, 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cFlY_SZaU6hd",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cFlY_SZaU6hd",
    "outputId": "d2700f62-8ce2-4dc9-df8f-3ba1f7ae2666"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GRU CV average: 0.5695652173913043\n",
      "Best Params:\n",
      "learning_rate: 1e-05\n",
      "epoch: 1000\n",
      "dropout: 0.3\n",
      "hidden_layer_size_1: 500\n",
      "hidden_layer_size_2: 64\n",
      "num_layer_gru 2\n",
      "batch_size_gru 16\n",
      "look_back_gru 15\n"
     ]
    }
   ],
   "source": [
    "# GRU\n",
    "# Chosing a suitable model : Using random search through parameters and Cross Validation\n",
    "# Random search through parameters combinations to find a suitable model\n",
    "learning_rate_set = [0.001, 0.0001, 0.00001, 0.000001]\n",
    "epoch_set = [500, 750, 1_000]\n",
    "dropout_set = [0.2, 0.3, 0.4]\n",
    "hidden_layer_size_1_set = [32, 64, 128, 256, 400, 500, 750]\n",
    "hidden_layer_size_2_set = [32, 64, 128, 256, 400, 500, 750]\n",
    "num_layers_set = [1, 2, 3]\n",
    "batch_size_set = [16, 32, 64, 128, 256]\n",
    "gnn_look_back_set = [1, 3, 5, 10, 15]\n",
    "\n",
    "already_generated = set()\n",
    "best_overall_val_acc = 0.0\n",
    "best_hyperparams = None\n",
    "\n",
    "for count in range(1, 15+1):\n",
    "    found = True\n",
    "    while found:\n",
    "        learning_rate = random.choice(learning_rate_set)\n",
    "        epoch = random.choice(epoch_set)\n",
    "        dropout = random.choice(dropout_set)\n",
    "        hidden_layer_1 = random.choice(hidden_layer_size_1_set)\n",
    "        hidden_layer_2 = random.choice(hidden_layer_size_2_set)\n",
    "        num_layer = random.choice(num_layers_set)\n",
    "        batch_size = random.choice(batch_size_set)\n",
    "        look_back = random.choice(gnn_look_back_set)\n",
    "        hyperparams = (learning_rate, epoch, dropout, hidden_layer_1, hidden_layer_2, num_layer, batch_size, look_back)\n",
    "        if hyperparams not in already_generated:\n",
    "            already_generated.add(hyperparams)\n",
    "            found = False\n",
    "    \n",
    "    # Training the model using k-fold validation\n",
    "    fold_val_accuracy = []\n",
    "    for fold, (train_index, val_index) in enumerate(tscv.split(X_tensor)):\n",
    "        X_train, X_val = X_tensor[train_index], X_tensor[val_index]\n",
    "        y_train, y_val = y_tensor[train_index], y_tensor[val_index]\n",
    "        # preprocess the data and convert them to dataloaders\n",
    "\n",
    "        # define the sequence length (how many days to use as input to the model)\n",
    "        seq_length = look_back\n",
    "\n",
    "        # create a sliding window view of the data for sequences of length seq_length\n",
    "        train_seq = [X_train[i:i+seq_length] for i in range(len(X_train) - seq_length)]\n",
    "        val_seq = [X_val[i:i+seq_length] for i in range(len(X_val) - seq_length)]\n",
    "\n",
    "        # convert to PyTorch tensors\n",
    "        train_seq = torch.stack(train_seq)\n",
    "        val_seq = torch.stack(val_seq)\n",
    "\n",
    "        # create PyTorch dataset and data loader\n",
    "        train_dataset = TensorDataset(train_seq, y_train[seq_length:])\n",
    "        val_dataset = TensorDataset(val_seq, y_val[seq_length:])\n",
    "\n",
    "        train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n",
    "        val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "        score = train_model_on_split_gru_or_lstm(train_dataloader, val_dataloader, hyperparams, input_size, 'GRU')\n",
    "        fold_val_accuracy.append(score)\n",
    "\n",
    "        avg_val_acc = sum(fold_val_accuracy)/len(fold_val_accuracy)\n",
    "             \n",
    "    # check if this param configuration had the best val accuracy\n",
    "    if avg_val_acc > best_overall_val_acc:\n",
    "        print(count,\"Found better accuracy of\",avg_val_acc)\n",
    "        best_overall_val_acc = avg_val_acc\n",
    "        best_hyperparams = hyperparams\n",
    "\n",
    "print(\"GRU CV average:\", best_overall_val_acc)\n",
    "# Best Params found for GRU\n",
    "(learning_rate_gru, epoch_gru, dropout_gru, hidden_layer_1_gru, hidden_layer_2_gru, num_layer_gru, batch_size_gru, look_back_gru) = best_hyperparams\n",
    "print('Best Params:')\n",
    "print('learning_rate:', learning_rate_gru)\n",
    "print('epoch:', epoch_gru)\n",
    "print('dropout:', dropout_gru) \n",
    "print('hidden_layer_size_1:', hidden_layer_1_gru)\n",
    "print('hidden_layer_size_2:', hidden_layer_2_gru)\n",
    "print('num_layer_gru', num_layer_gru)\n",
    "print('batch_size_gru', batch_size_gru)\n",
    "print('look_back_gru', look_back_gru)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "HI8Ai6WhRNGS",
   "metadata": {
    "id": "HI8Ai6WhRNGS"
   },
   "outputs": [],
   "source": [
    "(learning_rate_gru, epoch_gru, dropout_gru, hidden_layer_1_gru, hidden_layer_2_gru, num_layer_gru, batch_size_gru, look_back_gru) = (1e-05, 500, 0.4, 64, 500, 2, 64, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "-qznyAI78DUH",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-qznyAI78DUH",
    "outputId": "64a1bc6f-71dc-4647-88d1-222971aab981"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM CV average: 0.5717703349282297\n",
      "Best Params:\n",
      "learning_rate: 0.0001\n",
      "epoch: 500\n",
      "dropout: 0.3\n",
      "hidden_layer_size_1: 256\n",
      "hidden_layer_size_2: 500\n",
      "num_layer_lstm 1\n",
      "batch_size_lstm 64\n",
      "look_back_lstm 15\n"
     ]
    }
   ],
   "source": [
    "# LSTM\n",
    "# Chosing a suitable model : Using random search through parameters and Cross Validation\n",
    "# Random search through parameters combinations to find a suitable model\n",
    "learning_rate_set = [0.001, 0.0001, 0.00001, 0.000001]\n",
    "epoch_set = [500, 750, 1_000]\n",
    "dropout_set = [0.2, 0.3, 0.4]\n",
    "hidden_layer_size_1_set = [32, 64, 128, 256, 400, 500, 750]\n",
    "hidden_layer_size_2_set = [32, 64, 128, 256, 400, 500, 750]\n",
    "num_layers_set = [1,2,3]\n",
    "batch_size_set = [16, 32, 64, 128, 256]\n",
    "lstm_look_back_set = [1, 3, 5, 10, 15]\n",
    "\n",
    "already_generated = set()\n",
    "best_overall_val_acc = 0.0\n",
    "best_hyperparams = None\n",
    "\n",
    "for count in range(1, 15+1):\n",
    "    found = True\n",
    "    while found:\n",
    "        learning_rate = random.choice(learning_rate_set)\n",
    "        epoch = random.choice(epoch_set)\n",
    "        dropout = random.choice(dropout_set)\n",
    "        hidden_layer_1 = random.choice(hidden_layer_size_1_set)\n",
    "        hidden_layer_2 = random.choice(hidden_layer_size_2_set)\n",
    "        num_layer = random.choice(num_layers_set)\n",
    "        batch_size = random.choice(batch_size_set)\n",
    "        look_back_lstm = random.choice(lstm_look_back_set)\n",
    "\n",
    "        hyperparams = (learning_rate, epoch, dropout, hidden_layer_1, hidden_layer_2, num_layer, batch_size, look_back_lstm)\n",
    "        if hyperparams not in already_generated:\n",
    "            already_generated.add(hyperparams)\n",
    "            found = False\n",
    "    \n",
    "    # Training the model using k-fold validation\n",
    "    fold_val_accuracy = []\n",
    "    for fold, (train_index, val_index) in enumerate(tscv.split(X_tensor)):\n",
    "        X_train, X_val = X_tensor[train_index], X_tensor[val_index]\n",
    "        y_train, y_val = y_tensor[train_index], y_tensor[val_index]\n",
    "        # preprocess the data and convert them to dataloaders\n",
    "\n",
    "        # define the sequence length (how many days to use as input to the model)\n",
    "        seq_length = look_back_lstm\n",
    "\n",
    "        # create a sliding window view of the data for sequences of length seq_length\n",
    "        train_seq = [X_train[i:i+seq_length] for i in range(len(X_train) - seq_length)]\n",
    "        val_seq = [X_val[i:i+seq_length] for i in range(len(X_val) - seq_length)]\n",
    "\n",
    "        # convert to PyTorch tensors\n",
    "        train_seq = torch.stack(train_seq)\n",
    "        val_seq = torch.stack(val_seq)\n",
    "\n",
    "        # create PyTorch dataset and data loader\n",
    "        train_dataset = TensorDataset(train_seq, y_train[seq_length:])\n",
    "        val_dataset = TensorDataset(val_seq, y_val[seq_length:])\n",
    "\n",
    "        train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n",
    "        val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "        score = train_model_on_split_gru_or_lstm(train_dataloader, val_dataloader, hyperparams, input_size, 'LSTM')\n",
    "        fold_val_accuracy.append(score)\n",
    "\n",
    "        avg_val_acc = sum(fold_val_accuracy)/len(fold_val_accuracy)\n",
    "                        \n",
    "    # check if this param configuration had the best val accuracy\n",
    "    if avg_val_acc > best_overall_val_acc:\n",
    "        print(count,\"Found better accuracy of\",avg_val_acc)\n",
    "        best_overall_val_acc = avg_val_acc\n",
    "        best_hyperparams = hyperparams\n",
    "\n",
    "print(\"LSTM CV average:\" ,best_overall_val_acc)\n",
    "# Best Params found for LSTM network\n",
    "(learning_rate_lstm, epoch_lstm, dropout_lstm, hidden_layer_1_lstm, hidden_layer_2_lstm, num_layer_lstm, batch_size_lstm, look_back_lstm) = best_hyperparams\n",
    "print('Best Params:')\n",
    "print('learning_rate:', learning_rate_lstm)\n",
    "print('epoch:', epoch_lstm)\n",
    "print('dropout:', dropout_lstm) \n",
    "print('hidden_layer_size_1:', hidden_layer_1_lstm)\n",
    "print('hidden_layer_size_2:', hidden_layer_2_lstm)\n",
    "print('num_layer_lstm', num_layer_lstm)\n",
    "print('batch_size_lstm', batch_size_lstm)\n",
    "print('look_back_lstm', look_back_lstm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "JKHv_Qe4RvZo",
   "metadata": {
    "id": "JKHv_Qe4RvZo"
   },
   "outputs": [],
   "source": [
    "(learning_rate_lstm, epoch_lstm, dropout_lstm, hidden_layer_1_lstm, hidden_layer_2_lstm, num_layer_lstm, batch_size_lstm, look_back_lstm) = (1e-05, 750, 0.3, 750, 128, 1, 128, 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "pBfIcdJmjT6o",
   "metadata": {
    "id": "pBfIcdJmjT6o"
   },
   "outputs": [],
   "source": [
    "# function to generate results using all ML model\n",
    "def Generate_results(x_Train_set, y_Train_set, X_train, y_train, X_val, y_val, X_test, y_test):\n",
    "  # Logistic Regression\n",
    "    logisticRegr = LogisticRegression()\n",
    "    logisticRegr.fit(x_Train_set, y_Train_set)\n",
    "\n",
    "    print(\"Logistic Regression accuracy on train set is:\",logisticRegr.score(x_Train_set, y_Train_set))\n",
    "    print(\"Logistic Regression accuracy on test set is:\",logisticRegr.score(X_test, y_test))\n",
    "\n",
    "    # Random Forest Classifier\n",
    "    n_estimators_values = [64, 128, 246, 400, 500]\n",
    "    best_val_acc = 0.0\n",
    "    best_rf_model = None\n",
    "\n",
    "    for n_estimators in n_estimators_values:\n",
    "        rf_model = RandomForestClassifier(n_estimators=n_estimators, random_state=1)\n",
    "        rf_model.fit(X_train, y_train)\n",
    "\n",
    "        # Validation set accuracy\n",
    "        val_pred = rf_model.predict(X_val)\n",
    "        val_acc = accuracy_score(y_val, val_pred)\n",
    "\n",
    "        # Keep track of the best model\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            best_rf_model = rf_model\n",
    "\n",
    "\n",
    "    # Test set accuracy of the best model\n",
    "    test_pred = best_rf_model.predict(X_test)\n",
    "    test_acc = accuracy_score(y_test, test_pred)\n",
    "    print(f\"Best RF model: n_estimators={best_rf_model.n_estimators}, validation set accuracy={best_val_acc}, test set accuracy={test_acc}\")\n",
    "\n",
    "    # SVM\n",
    "    # Define the hyperparameter grid to search over parameters\n",
    "    param_grid = {'kernel': ['linear', 'rbf'], 'C': [0.1, 1, 10], 'gamma': [0.1, 1, 10]}\n",
    "\n",
    "    # Create the SVM model\n",
    "    svm_model = SVC()\n",
    "\n",
    "    # GridSearchCV object\n",
    "    grid_search = GridSearchCV(svm_model, param_grid, cv=5)\n",
    "\n",
    "    # Fit to the training data\n",
    "    grid_search.fit(x_Train_set, y_Train_set)\n",
    "\n",
    "    # Print the best hyperparameters and corresponding score\n",
    "    print(\"SVM: Best parameters:\", grid_search.best_params_)\n",
    "    print(\"SVM: Best CV avergae score:\", grid_search.best_score_)\n",
    "\n",
    "    # Use the best model to predict the test set\n",
    "    best_svm_model = grid_search.best_estimator_\n",
    "    best_svm_model.fit(x_Train_set, y_Train_set)\n",
    "\n",
    "    test_acc = best_svm_model.predict(X_test)\n",
    "    test_acc = accuracy_score(y_test, test_acc)\n",
    "    print(\"SVM: Accuracy on Test set:\", test_acc)\n",
    "\n",
    "    # Convert current dataset split to tensors\n",
    "    X_Train = torch.tensor(X_train.values, dtype=torch.float32, device=device)\n",
    "    y_Train = torch.tensor(y_train.values, dtype=torch.float32, device=device)\n",
    "\n",
    "    X_Val = torch.tensor(X_val.values, dtype=torch.float32, device=device)\n",
    "    y_Val = torch.tensor(y_val.values, dtype=torch.float32, device=device)\n",
    "\n",
    "    X_Test = torch.tensor(X_test.values, dtype=torch.float32, device=device)\n",
    "    y_Test = torch.tensor(y_test.values, dtype=torch.float32, device=device)\n",
    "\n",
    "    # ANN\n",
    "    # Perform Early stopping on the best found parameter option (do this 3 times and keep the best found model)\n",
    "    # Define the optimizer\n",
    "    train_error = []\n",
    "    train_accs = []\n",
    "    val_accs = []\n",
    "    best_val_acc = 0.0\n",
    "    for run in range(0, 3):\n",
    "        model_ANN = ANN(len(X_Train[0]), hidden_layer_ann,  dropout_ann)\n",
    "        model_ANN = model_ANN.to(device)\n",
    "        optimiser = torch.optim.Adam(model_ANN.parameters(), lr = learning_rate_ann)\n",
    "\n",
    "        for step in range(1, epoch_ann+1):\n",
    "            # Forward pass\n",
    "            output = model_ANN(X_Train)\n",
    "            error = torch.nn.functional.binary_cross_entropy_with_logits(output, y_Train.unsqueeze(1))\n",
    "\n",
    "            # Backward pass\n",
    "            optimiser.zero_grad()\n",
    "            error.backward()\n",
    "            optimiser.step()\n",
    "\n",
    "            # Test Accuracy on training and validation sets\n",
    "            model_ANN.eval()\n",
    "            with torch.no_grad():\n",
    "                # train set\n",
    "                pred = torch.round(torch.sigmoid(model_ANN(X_Train)))[:, 0].detach().cpu().numpy()\n",
    "                train_acc = accuracy_score(pred, y_Train.cpu())\n",
    "\n",
    "                # get validation and test set accuracy (when model learns train set with > 0.5 accuracy)\n",
    "                if train_acc > 0.5:\n",
    "                    # validation set\n",
    "                    predictions = torch.round(torch.sigmoid(model_ANN(X_Val)))[:, 0].detach().cpu().numpy()\n",
    "                    val_acc = accuracy_score(y_Val.cpu(), predictions)\n",
    "                    if val_acc > best_val_acc:\n",
    "                        best_val_acc = val_acc\n",
    "                        # Save the model\n",
    "                        torch.save(model_ANN.state_dict(), 'ann_best_model.pth')\n",
    "                    if train_acc > 0.99:\n",
    "                        break\n",
    "            model_ANN.train()\n",
    "    # Loading the best model\n",
    "    best_ann_model = ANN(len(X_Train[0]), hidden_layer_ann, dropout_ann)\n",
    "    best_ann_model.load_state_dict(torch.load('ann_best_model.pth'))\n",
    "\n",
    "    best_ann_model = best_ann_model.to(device)\n",
    "    best_ann_model.eval()\n",
    "    # accuracy of training set\n",
    "    with torch.no_grad():\n",
    "        # train set\n",
    "        pred = torch.round(torch.sigmoid(best_ann_model(X_Train)))[:, 0].detach().cpu().numpy()\n",
    "        test_acc = accuracy_score(pred, y_Train.cpu())\n",
    "        print(\"ANN: Training Accuracy:\", test_acc)\n",
    "\n",
    "    # Accuracy on Validation set\n",
    "    with torch.no_grad():\n",
    "        # val set\n",
    "        predictions = torch.round(torch.sigmoid(best_ann_model(X_Val)))[:, 0].detach().cpu().numpy()\n",
    "        val_acc = accuracy_score(predictions, y_Val.cpu())\n",
    "        print(\"ANN: Validation Accuracy:\", val_acc)\n",
    "\n",
    "    # Accuracy on Test set\n",
    "    with torch.no_grad():\n",
    "        # test set\n",
    "        pred = torch.round(torch.sigmoid(best_ann_model(X_Test)))[:, 0].detach().cpu().numpy()\n",
    "        test_acc = accuracy_score(pred, y_test)\n",
    "        print(\"ANN: Test Accuracy:\", test_acc)\n",
    "\n",
    "\n",
    "    # Convert current dataset split to tensors\n",
    "    X_train = torch.tensor(X_train.values, dtype=torch.float32, device=device)\n",
    "    y_train = torch.tensor(y_train.values, dtype=torch.float32, device=device)\n",
    "\n",
    "    X_val = torch.tensor(X_val.values, dtype=torch.float32, device=device)\n",
    "    y_val = torch.tensor(y_val.values, dtype=torch.float32, device=device)\n",
    "\n",
    "    X_test = torch.tensor(X_test.values, dtype=torch.float32, device=device)\n",
    "    y_test = torch.tensor(y_test.values, dtype=torch.float32, device=device)\n",
    "\n",
    "    # define the sequence length (how many days to use as input to the model)\n",
    "    seq_length = look_back_gru\n",
    "\n",
    "    # make the validation and test sets the actual sets by taking care of the lookback\n",
    "    X_val = torch.cat([X_train[-seq_length:], X_val], dim=0)\n",
    "    y_val = torch.cat([y_train[-seq_length:], y_val], dim=0)\n",
    "\n",
    "    X_test = torch.cat([X_val[-seq_length:], X_test], dim=0)\n",
    "    y_test = torch.cat([y_val[-seq_length:], y_test], dim=0)\n",
    "\n",
    "\n",
    "    # create a sliding window view of the data for sequences of length seq_length\n",
    "    train_seq = [X_train[i:i+seq_length] for i in range(len(X_train) - seq_length)]\n",
    "    val_seq = [X_val[i:i+seq_length] for i in range(len(X_val) - seq_length)]\n",
    "    test_seq = [X_test[i:i+seq_length] for i in range(len(X_test) - seq_length)]\n",
    "\n",
    "    # convert to PyTorch tensors\n",
    "    train_seq = torch.stack(train_seq)\n",
    "    val_seq = torch.stack(val_seq)\n",
    "    test_seq = torch.stack(test_seq)\n",
    "\n",
    "    # create PyTorch dataset and data loader\n",
    "    train_dataset = TensorDataset(train_seq, y_train[seq_length:])\n",
    "    val_dataset = TensorDataset(val_seq, y_val[seq_length:])\n",
    "    test_dataset = TensorDataset(test_seq, y_test[seq_length:])\n",
    "\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=batch_size_gru, shuffle=False)\n",
    "    val_dataloader = DataLoader(val_dataset, batch_size=batch_size_gru, shuffle=False)\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=batch_size_gru, shuffle=False)\n",
    "\n",
    "    # get all train data into inputs and labels tensors\n",
    "    train_inputs = torch.empty(0, device=device)\n",
    "    train_labels = torch.empty(0, device=device)\n",
    "    for inputs, labels in train_dataloader:\n",
    "        train_inputs = torch.cat([train_inputs, inputs], dim=0)\n",
    "        train_labels = torch.cat([train_labels, labels], dim=0)\n",
    "\n",
    "    # get all validation data into inputs and labels tensors\n",
    "    val_inputs = torch.empty(0, device=device)\n",
    "    val_labels = torch.empty(0, device=device)\n",
    "    for inputs, labels in val_dataloader:\n",
    "        val_inputs = torch.cat([val_inputs, inputs], dim=0)\n",
    "        val_labels = torch.cat([val_labels, labels], dim=0)\n",
    "\n",
    "    # get all test data into inputs and labels tensors\n",
    "    test_inputs = torch.empty(0, device=device)\n",
    "    test_labels = torch.empty(0, device=device)\n",
    "    for inputs, labels in test_dataloader:\n",
    "        test_inputs = torch.cat([test_inputs, inputs], dim=0)\n",
    "        test_labels = torch.cat([test_labels, labels], dim=0)\n",
    "\n",
    "    # GRU\n",
    "    # Perform Early stopping on the best found parameter option (do this 3 times and keep the best found model)\n",
    "    best_val_acc = 0.0\n",
    "    for run in range(0, 3):\n",
    "        # Instantiate the model\n",
    "        model = GRU_Neural_Network(input_size, hidden_layer_1_gru, hidden_layer_2_gru, dropout_gru, num_layer_gru)\n",
    "        model = model.to(device)\n",
    "\n",
    "\n",
    "        # Define the optimiser\n",
    "        optimiser = torch.optim.Adam(model.parameters(), lr=learning_rate_gru)\n",
    "\n",
    "        # train on the split\n",
    "        # train the model\n",
    "        for step in range(epoch_gru):\n",
    "            model.train()\n",
    "            # train on batches\n",
    "            for inputs, labels in train_dataloader:\n",
    "                optimiser.zero_grad()\n",
    "                outputs = model(inputs)\n",
    "                loss = torch.nn.functional.binary_cross_entropy_with_logits(outputs, labels.unsqueeze(1))\n",
    "                loss.backward()\n",
    "                optimiser.step()\n",
    "            # check accuracy on train set and validation set\n",
    "            # Test Accuracy on training and validation sets\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                # train set\n",
    "                pred = torch.round(torch.sigmoid(model(train_inputs)))[:, 0].detach().cpu().numpy()\n",
    "                train_acc = accuracy_score(pred, train_labels.cpu())\n",
    "\n",
    "                # get validation and test set accuracy (when model learns train set with > 0.5 accuracy)\n",
    "                if train_acc > 0.5:\n",
    "                    # validation set\n",
    "                    predictions = torch.round(torch.sigmoid(model(val_inputs)))[:, 0].detach().cpu().numpy()\n",
    "                    val_acc = accuracy_score(predictions, val_labels.cpu())\n",
    "                    if val_acc > best_val_acc:\n",
    "                        best_val_acc = val_acc\n",
    "                        # Save the model\n",
    "                        torch.save(model.state_dict(), 'gru_best_model.pth')\n",
    "            if train_acc > 0.99:\n",
    "                break\n",
    "        model.train()\n",
    "    # Loading the best model\n",
    "    best_gru_model = GRU_Neural_Network(len(X_train[0]), hidden_layer_1_gru, hidden_layer_2_gru, dropout_gru, num_layer_gru)\n",
    "    best_gru_model.load_state_dict(torch.load('gru_best_model.pth'))\n",
    "\n",
    "    best_gru_model = best_gru_model.to(device)\n",
    "    best_gru_model.eval()\n",
    "    # accuracy of training set\n",
    "    with torch.no_grad():\n",
    "        # train set\n",
    "        pred = torch.round(torch.sigmoid(best_gru_model(train_inputs)))[:, 0].detach().cpu().numpy()\n",
    "        test_acc = accuracy_score(pred, train_labels.cpu())\n",
    "        print(\"GRU Training Accuracy:\", test_acc)\n",
    "\n",
    "    # Accuracy on Validation set\n",
    "    with torch.no_grad():\n",
    "        # val set\n",
    "        predictions = torch.round(torch.sigmoid(best_gru_model(val_inputs)))[:, 0].detach().cpu().numpy()\n",
    "        val_acc = accuracy_score(predictions, val_labels.cpu())\n",
    "        print(\"GRU Validation Accuracy:\", val_acc)\n",
    "\n",
    "    # Accuracy on Test set\n",
    "    with torch.no_grad():\n",
    "        # test set\n",
    "        pred = torch.round(torch.sigmoid(best_gru_model(test_inputs)))[:, 0].detach().cpu().numpy()\n",
    "        test_acc = accuracy_score(pred, test_labels.cpu())\n",
    "        print(\"GRU Test Accuracy:\",test_acc)\n",
    "\n",
    "    # LSTM\n",
    "    # define the sequence length (how many days to use as input to the model)\n",
    "    seq_length = look_back_lstm\n",
    "\n",
    "    # concat the validation and test sets according to the lookback\n",
    "    X_val = torch.cat([X_train[-seq_length:], X_val], dim=0)\n",
    "    y_val = torch.cat([y_train[-seq_length:], y_val], dim=0)\n",
    "\n",
    "    X_test = torch.cat([X_val[-seq_length:], X_test], dim=0)\n",
    "    y_test = torch.cat([y_val[-seq_length:], y_test], dim=0)\n",
    "\n",
    "\n",
    "    # create a sliding window view of the data for sequences of length seq_length\n",
    "    train_seq = [X_train[i:i+seq_length] for i in range(len(X_train) - seq_length)]\n",
    "    val_seq = [X_val[i:i+seq_length] for i in range(len(X_val) - seq_length)]\n",
    "    test_seq = [X_test[i:i+seq_length] for i in range(len(X_test) - seq_length)]\n",
    "\n",
    "    # convert to PyTorch tensors\n",
    "    train_seq = torch.stack(train_seq)\n",
    "    val_seq = torch.stack(val_seq)\n",
    "    test_seq = torch.stack(test_seq)\n",
    "\n",
    "    # create PyTorch dataset and data loader\n",
    "    train_dataset = TensorDataset(train_seq, y_train[seq_length:])\n",
    "    val_dataset = TensorDataset(val_seq, y_val[seq_length:])\n",
    "    test_dataset = TensorDataset(test_seq, y_test[seq_length:])\n",
    "\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=batch_size_lstm, shuffle=False)\n",
    "    val_dataloader = DataLoader(val_dataset, batch_size=batch_size_lstm, shuffle=False)\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=batch_size_lstm, shuffle=False)\n",
    "\n",
    "    # get all train data into inputs and labels tensors\n",
    "    train_inputs = torch.empty(0, device=device)\n",
    "    train_labels = torch.empty(0, device=device)\n",
    "    for inputs, labels in train_dataloader:\n",
    "        train_inputs = torch.cat([train_inputs, inputs], dim=0)\n",
    "        train_labels = torch.cat([train_labels, labels], dim=0)\n",
    "\n",
    "    # get all validation data into inputs and labels tensors\n",
    "    val_inputs = torch.empty(0, device=device)\n",
    "    val_labels = torch.empty(0, device=device)\n",
    "    for inputs, labels in val_dataloader:\n",
    "        val_inputs = torch.cat([val_inputs, inputs], dim=0)\n",
    "        val_labels = torch.cat([val_labels, labels], dim=0)\n",
    "\n",
    "    # get all test data into inputs and labels tensors\n",
    "    test_inputs = torch.empty(0, device=device)\n",
    "    test_labels = torch.empty(0, device=device)\n",
    "    for inputs, labels in test_dataloader:\n",
    "        test_inputs = torch.cat([test_inputs, inputs], dim=0)\n",
    "        test_labels = torch.cat([test_labels, labels], dim=0)\n",
    "\n",
    "    # LSTM\n",
    "    # Perform Early stopping on the best found parameter option (do this 3 times and keep the best found model)\n",
    "    best_val_acc = 0.0\n",
    "    for run in range(0, 3):\n",
    "        # Instantiate the model\n",
    "        model = LSTM_Neural_Network(input_size, hidden_layer_1_lstm, hidden_layer_2_lstm, dropout_lstm, num_layer_lstm)\n",
    "        model = model.to(device)\n",
    "\n",
    "\n",
    "        # Define the optimiser\n",
    "        optimiser = torch.optim.Adam(model.parameters(), lr=learning_rate_lstm)\n",
    "\n",
    "        # train on the split\n",
    "        # train the model\n",
    "        for step in range(epoch_lstm):\n",
    "            model.train()\n",
    "            # train on batches\n",
    "            for inputs, labels in train_dataloader:\n",
    "                optimiser.zero_grad()\n",
    "                outputs = model(inputs)\n",
    "                loss = torch.nn.functional.binary_cross_entropy_with_logits(outputs, labels.unsqueeze(1))\n",
    "                loss.backward()\n",
    "                optimiser.step()\n",
    "            # check accuracy on train set and validation set\n",
    "            # Test Accuracy on training and validation sets\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                # train set\n",
    "                pred = torch.round(torch.sigmoid(model(train_inputs)))[:, 0].detach().cpu().numpy()\n",
    "                train_acc = accuracy_score(pred, train_labels.cpu())\n",
    "\n",
    "                # get validation and test set accuracy (when model learns train set with > 0.5 accuracy)\n",
    "                if train_acc > 0.5:\n",
    "                    # validation set\n",
    "                    predictions = torch.round(torch.sigmoid(model(val_inputs)))[:, 0].detach().cpu().numpy()\n",
    "                    val_acc = accuracy_score(predictions, val_labels.cpu())\n",
    "                    if val_acc > best_val_acc:\n",
    "                        best_val_acc = val_acc\n",
    "                        # Save the model\n",
    "                        torch.save(model.state_dict(), 'lstm_best_model.pth')\n",
    "                if train_acc > 0.99:\n",
    "                    break\n",
    "            model.train()\n",
    "    # Loading the best model\n",
    "    best_lstm_model = LSTM_Neural_Network(len(X_train[0]), hidden_layer_1_lstm, hidden_layer_2_lstm, dropout_lstm, num_layer_lstm)\n",
    "    best_lstm_model.load_state_dict(torch.load('lstm_best_model.pth'))\n",
    "\n",
    "    best_lstm_model = best_lstm_model.to(device)\n",
    "    best_lstm_model.eval()\n",
    "    # accuracy of training set\n",
    "    with torch.no_grad():\n",
    "        # train set\n",
    "        pred = torch.round(torch.sigmoid(best_lstm_model(train_inputs)))[:, 0].detach().cpu().numpy()\n",
    "        test_acc = accuracy_score(pred, train_labels.cpu())\n",
    "        print(\"LSTM Training Accuracy:\", test_acc)\n",
    "\n",
    "    # Accuracy on Validation set\n",
    "    with torch.no_grad():\n",
    "        # val set\n",
    "        predictions = torch.round(torch.sigmoid(best_lstm_model(val_inputs)))[:, 0].detach().cpu().numpy()\n",
    "        val_acc = accuracy_score(predictions, val_labels.cpu())\n",
    "        print(\"LSTM Validation Accuracy:\", val_acc)\n",
    "\n",
    "    # Accuracy on Test set\n",
    "    with torch.no_grad():\n",
    "        # test set\n",
    "        pred = torch.round(torch.sigmoid(best_lstm_model(test_inputs)))[:, 0].detach().cpu().numpy()\n",
    "        test_acc = accuracy_score(pred, test_labels.cpu())\n",
    "        print(\"LSTM Test Accuracy:\",test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "v6nnOidbUr-4",
   "metadata": {
    "id": "v6nnOidbUr-4"
   },
   "outputs": [],
   "source": [
    "# Keep trend baseline\n",
    "# take the mean value from last 3 days. Since the list contains just 0 or 1 we can do this easily.\n",
    "# populate test set using keep trend baseline\n",
    "def using_keeptrend(y_train, y_val, y_test, ans, n_days):\n",
    "    index = y_train\n",
    "    pred_val = []\n",
    "    # generate validation set\n",
    "    while index < y_train + y_val:\n",
    "        last_ndays = list(ans[index-n_days : index])\n",
    "        pred_val.append(round(sum(last_ndays)/len(last_ndays)))\n",
    "        index = index + 1\n",
    "        \n",
    "    pred_test = []\n",
    "    # generate test set\n",
    "    while index < y_train + y_val + y_test:\n",
    "        last_ndays = list(ans[index-n_days : index])\n",
    "        pred_test.append(round(sum(last_ndays)/len(last_ndays)))\n",
    "        index = index + 1\n",
    "    return pred_val, pred_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "z7tx8KQ0HpRp",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 269
    },
    "id": "z7tx8KQ0HpRp",
    "outputId": "54f61986-ceca-4835-d1a0-b19e8989ab39"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "*** Results for Train: 2013-01-01 -> 2019-12-31 Test: 2020-01-01 -> 2020-12-31 ***\n",
      "Baseline: Keep Trend model, Val Accuracy: 0.4519774011299435\n",
      "Baseline: Keep Trend model, Test Accuracy: 0.541501976284585\n",
      "\n",
      "*** Results for Train: 2013-01-01 -> 2020-12-31 Test: 2021-01-01 -> 2021-12-31 ***\n",
      "Baseline: Keep Trend model, Val Accuracy: 0.49504950495049505\n",
      "Baseline: Keep Trend model, Test Accuracy: 0.5119047619047619\n",
      "\n",
      "*** Results for Train: 2013-01-01 -> 2021-12-31 Test: 2022-01-01 -> 2022-12-31 ***\n",
      "Baseline: Keep Trend model, Val Accuracy: 0.4933920704845815\n",
      "Baseline: Keep Trend model, Test Accuracy: 0.5099601593625498\n"
     ]
    }
   ],
   "source": [
    "# Baseline for each year\n",
    "i = 0\n",
    "while i < len(Train_Test_Sets):\n",
    "    print(\"\\n*** Results for Train:\",train_test_splits[i][0],'->',train_test_splits[i][1],'Test:',train_test_splits[i][2],'->',train_test_splits[i][3],\"***\")\n",
    "    # Split the train set to have a validation set of 10% of the train data\n",
    "    X_train, X_val, y_train, y_val = train_test_split(Train_Test_Sets[i][0].drop(columns=['Label']),\n",
    "                                                      Train_Test_Sets[i][0]['Label'], test_size=0.10 , shuffle=False, random_state=0)\n",
    "\n",
    "    # Assign the test set\n",
    "    X_test = Train_Test_Sets[i][1].drop(columns=['Label'])\n",
    "    y_test = Train_Test_Sets[i][1]['Label']\n",
    "\n",
    "    # Baseline\n",
    "    keeptrend_val, keeptrend_test = using_keeptrend(len(y_train), len(y_val), 0, stock_data['Tommo_Direction'], 3)\n",
    "    val_acc = accuracy_score(keeptrend_val, y_val)\n",
    "    print(\"Baseline: Keep Trend model, Val Accuracy:\", val_acc)\n",
    "    keeptrend_test, keeptrend_test = using_keeptrend(len(y_test), 0, len(y_test), stock_data['Tommo_Direction'], 3)\n",
    "    test_acc = accuracy_score(keeptrend_test, y_test)\n",
    "    print(\"Baseline: Keep Trend model, Test Accuracy:\", test_acc)\n",
    "    i = i + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "U8BtdAdiUw_v",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "U8BtdAdiUw_v",
    "outputId": "d29cc9d9-9df6-4c5e-a3bb-72d72b822711"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Results for Train: 2013-01-01 -> 2019-12-31 Test: 2020-01-01 -> 2020-12-31 ***\n",
      "Logistic Regression accuracy on train set is: 0.7927314026121521\n",
      "Logistic Regression accuracy on test set is: 0.5138339920948617\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_15084\\625648164.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[1;31m# ML models results\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m     \u001b[0mGenerate_results\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_Train_set\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_Train_set\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'\\n\\n'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_15084\\1633683993.py\u001b[0m in \u001b[0;36mGenerate_results\u001b[1;34m(x_Train_set, y_Train_set, X_train, y_train, X_val, y_val, X_test, y_test)\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mn_estimators\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mn_estimators_values\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m         \u001b[0mrf_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mRandomForestClassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_estimators\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mn_estimators\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m         \u001b[0mrf_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m         \u001b[1;31m# Validation set accuracy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    448\u001b[0m             \u001b[1;31m# parallel_backend contexts set at a higher level,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    449\u001b[0m             \u001b[1;31m# since correctness does not rely on using threads.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 450\u001b[1;33m             trees = Parallel(\n\u001b[0m\u001b[0;32m    451\u001b[0m                 \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    452\u001b[0m                 \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1044\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1045\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1046\u001b[1;33m             \u001b[1;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1047\u001b[0m                 \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1048\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    859\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    860\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 861\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    862\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    863\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    777\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    778\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 779\u001b[1;33m             \u001b[0mjob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    780\u001b[0m             \u001b[1;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    781\u001b[0m             \u001b[1;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[1;34m(self, func, callback)\u001b[0m\n\u001b[0;32m    206\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    207\u001b[0m         \u001b[1;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 208\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    209\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    210\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    570\u001b[0m         \u001b[1;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    571\u001b[0m         \u001b[1;31m# arguments in memory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 572\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    573\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    574\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    260\u001b[0m         \u001b[1;31m# change the default number of processes to -1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    261\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 262\u001b[1;33m             return [func(*args, **kwargs)\n\u001b[0m\u001b[0;32m    263\u001b[0m                     for func, args, kwargs in self.items]\n\u001b[0;32m    264\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    260\u001b[0m         \u001b[1;31m# change the default number of processes to -1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    261\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 262\u001b[1;33m             return [func(*args, **kwargs)\n\u001b[0m\u001b[0;32m    263\u001b[0m                     for func, args, kwargs in self.items]\n\u001b[0;32m    264\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\fixes.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    214\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    215\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mconfig_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 216\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    217\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    218\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\u001b[0m in \u001b[0;36m_parallel_build_trees\u001b[1;34m(tree, forest, X, y, sample_weight, tree_idx, n_trees, verbose, class_weight, n_samples_bootstrap)\u001b[0m\n\u001b[0;32m    183\u001b[0m             \u001b[0mcurr_sample_weight\u001b[0m \u001b[1;33m*=\u001b[0m \u001b[0mcompute_sample_weight\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"balanced\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mindices\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    184\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 185\u001b[1;33m         \u001b[0mtree\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcurr_sample_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    186\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    187\u001b[0m         \u001b[0mtree\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\tree\\_classes.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[0;32m    935\u001b[0m         \"\"\"\n\u001b[0;32m    936\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 937\u001b[1;33m         super().fit(\n\u001b[0m\u001b[0;32m    938\u001b[0m             \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    939\u001b[0m             \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\tree\\_classes.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[0;32m    418\u001b[0m             )\n\u001b[0;32m    419\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 420\u001b[1;33m         \u001b[0mbuilder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtree_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    421\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    422\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_outputs_\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mis_classifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "while i < len(Train_Test_Sets):\n",
    "    print(\"*** Results for Train:\",train_test_splits[i][0],'->',train_test_splits[i][1],'Test:',train_test_splits[i][2],'->',train_test_splits[i][3],\"***\")\n",
    "    # Split the train set to have a validation set of 10% of the train data\n",
    "    X_train, X_val, y_train, y_val = train_test_split(Train_Test_Sets[i][0].drop(columns=['Label']),\n",
    "                                                      Train_Test_Sets[i][0]['Label'], test_size=0.10 , shuffle=False, random_state=0)\n",
    "    x_Train_set = Train_Test_Sets[i][0].drop(columns=['Label'])\n",
    "    y_Train_set = Train_Test_Sets[i][0]['Label']\n",
    "\n",
    "    # Assign the test set\n",
    "    X_test = Train_Test_Sets[i][1].drop(columns=['Label'])\n",
    "    y_test = Train_Test_Sets[i][1]['Label']\n",
    "\n",
    "\n",
    "    # ML models results\n",
    "    Generate_results(x_Train_set, y_Train_set, X_train, y_train, X_val, y_val, X_test, y_test)\n",
    "    print('\\n\\n')\n",
    "\n",
    "    i = i + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "GIkJSpR6qjEm",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GIkJSpR6qjEm",
    "outputId": "2cfb9138-d958-4b10-d824-6de825e568df"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Results for Train: 2013-01-01 -> 2019-12-31 Test: 2020-01-01 -> 2020-12-31 ***\n",
      "Test set:\n",
      "UP trading days 145\n",
      "Down trading days 108\n",
      "*** Results for Train: 2013-01-01 -> 2020-12-31 Test: 2021-01-01 -> 2021-12-31 ***\n",
      "Test set:\n",
      "UP trading days 138\n",
      "Down trading days 114\n",
      "*** Results for Train: 2013-01-01 -> 2021-12-31 Test: 2022-01-01 -> 2022-12-31 ***\n",
      "Test set:\n",
      "UP trading days 119\n",
      "Down trading days 132\n"
     ]
    }
   ],
   "source": [
    "# How Balanced is the train and test sets are\n",
    "i = 0\n",
    "while i < len(Train_Test_Sets):\n",
    "    print(\"*** Results for Train:\",train_test_splits[i][0],'->',train_test_splits[i][1],'Test:',train_test_splits[i][2],'->',train_test_splits[i][3],\"***\")\n",
    "    # Split the train set to have a validation set of 10% of the train data\n",
    "    X_train, X_val, y_train, y_val = train_test_split(Train_Test_Sets[i][0].drop(columns=['Label']),\n",
    "                                                      Train_Test_Sets[i][0]['Label'], test_size=0.10 , shuffle=False, random_state=0)\n",
    "    x_Train_set = Train_Test_Sets[i][0].drop(columns=['Label'])\n",
    "    y_Train_set = Train_Test_Sets[i][0]['Label']\n",
    "\n",
    "    # Assign the test set\n",
    "    X_test = Train_Test_Sets[i][1].drop(columns=['Label'])\n",
    "    y_test = Train_Test_Sets[i][1]['Label']\n",
    "\n",
    "\n",
    "    # UP and Down Counts for test set\n",
    "    count_1 = sum(y_test == 1)\n",
    "    count_0 = sum(y_test == 0)\n",
    "    print('Test set:')\n",
    "    print(\"UP trading days\", count_1)\n",
    "    print(\"Down trading days\", count_0)\n",
    "\n",
    "\n",
    "    # UP and Down Counts for train set\n",
    "    #count_1 = sum(y_Train_set == 1)\n",
    "    #count_0 = sum(y_Train_set == 0)\n",
    "    #print('Train set:')\n",
    "    #print(\"UP trading days\", count_1)\n",
    "    #print(\"Down trading days\", count_0)\n",
    "    #print('\\n\\n')\n",
    "\n",
    "    i = i + 1"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
